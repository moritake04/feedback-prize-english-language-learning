{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tes = pd.read_csv(\"../../data/input/train_with_meta.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tes.iloc[:, 8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_0 = pd.read_csv(\"../../data/pseudo/pseudo_fold_0.csv\")\n",
    "pseudo_1 = pd.read_csv(\"../../data/pseudo/pseudo_fold_1.csv\")\n",
    "pseudo_2 = pd.read_csv(\"../../data/pseudo/pseudo_fold_2.csv\")\n",
    "pseudo_3 = pd.read_csv(\"../../data/pseudo/pseudo_fold_3.csv\")\n",
    "pseudo_4 = pd.read_csv(\"../../data/pseudo/pseudo_fold_4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_0[\"cohesion\"] = (pseudo_0[\"cohesion\"] + pseudo_1[\"cohesion\"] + pseudo_2[\"cohesion\"] + pseudo_3[\"cohesion\"] + pseudo_4[\"cohesion\"]) / 5  \n",
    "pseudo_0[\"syntax\"] = (pseudo_0[\"syntax\"] + pseudo_1[\"syntax\"] + pseudo_2[\"syntax\"] + pseudo_3[\"syntax\"] + pseudo_4[\"syntax\"]) / 5\n",
    "pseudo_0[\"vocabulary\"] = (pseudo_0[\"vocabulary\"] + pseudo_1[\"vocabulary\"] + pseudo_2[\"vocabulary\"] + pseudo_3[\"vocabulary\"] + pseudo_4[\"vocabulary\"]) / 5\n",
    "pseudo_0[\"phraseology\"] = (pseudo_0[\"phraseology\"] + pseudo_1[\"phraseology\"] + pseudo_2[\"phraseology\"] + pseudo_3[\"phraseology\"] + pseudo_4[\"phraseology\"]) / 5\n",
    "pseudo_0[\"grammar\"] = (pseudo_0[\"grammar\"] + pseudo_1[\"grammar\"] + pseudo_2[\"grammar\"] + pseudo_3[\"grammar\"] + pseudo_4[\"grammar\"]) / 5\n",
    "pseudo_0[\"conventions\"] = (pseudo_0[\"conventions\"] + pseudo_1[\"conventions\"] + pseudo_2[\"conventions\"] + pseudo_3[\"conventions\"] + pseudo_4[\"conventions\"]) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_0.to_csv(\"../../data/pseudo/pseudo_fold_mean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"../../data/pseudo/pseudo_fold_mean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_0[\"mean\"] = np.mean(pseudo_0.iloc[:,2:], axis=1)\n",
    "pseudo_1[\"mean\"] = np.mean(pseudo_1.iloc[:,2:], axis=1)\n",
    "pseudo_2[\"mean\"] = np.mean(pseudo_2.iloc[:,2:], axis=1)\n",
    "pseudo_3[\"mean\"] = np.mean(pseudo_3.iloc[:,2:], axis=1)\n",
    "pseudo_4[\"mean\"] = np.mean(pseudo_4.iloc[:,2:], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.concat([pseudo_0[\"mean\"], pseudo_1[\"mean\"], pseudo_2[\"mean\"], pseudo_3[\"mean\"], pseudo_4[\"mean\"]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "describe = summary.T.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for i in describe.iloc[2]:\n",
    "    if i <= 0.01:\n",
    "        print(i)\n",
    "        cnt += 1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, bins = np.histogram(describe.iloc[2], bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for i in range(1, len(bins)):\n",
    "    X.append((bins[i-1]+bins[i])/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(describe.iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_of_mean = (pseudo_0[\"mean\"] + pseudo_1[\"mean\"] + pseudo_2[\"mean\"] + pseudo_3[\"mean\"] + pseudo_4[\"mean\"]) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    AddedToken\n",
    ")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_features(df, col):\n",
    "    df[f\"{col}_num_words\"] = df[col].apply(\n",
    "        lambda x: len(str(x).split())\n",
    "    )  # num_words count\n",
    "\n",
    "    df[f\"{col}_num_unique_words\"] = df[col].apply(\n",
    "        lambda x: len(set(str(x).split()))\n",
    "    )  # num_unique_words count\n",
    "\n",
    "    df[f\"{col}_num_chars\"] = df[col].apply(lambda x: len(str(x)))  # num_chars count\n",
    "\n",
    "    \"\"\"\n",
    "    df[f\"{col}_num_stopwords\"] = df[col].apply(\n",
    "        lambda x: len(\n",
    "            [w for w in str(x).lower().split() if w in stopwords.words(\"english\")]\n",
    "        )\n",
    "    )  # stopword count\n",
    "    \"\"\"\n",
    "\n",
    "    df[f\"{col}_num_punctuations\"] = df[col].apply(\n",
    "        lambda x: len([c for c in str(x) if c in list(string.punctuation)])\n",
    "    )  # num_punctuations count 句読点\n",
    "\n",
    "    df[f\"{col}_num_words_upper\"] = df[col].apply(\n",
    "        lambda x: len([w for w in str(x) if w.isupper()])\n",
    "    )  # num_words_upper count 大文字\n",
    "\n",
    "    df[f\"{col}_mean_word_len\"] = df[col].apply(\n",
    "        lambda x: np.mean([len(w) for w in x.split()])\n",
    "    )  # mean_word_len\n",
    "\n",
    "    df[f\"{col}_num_paragraphs\"] = df[col].apply(\n",
    "        lambda x: len(list(filter(None, re.split(\"\\n\\n|\\r\\n|\\r|\\n\", x))))\n",
    "    )  # num_paragraphs count\n",
    "\n",
    "    \"\"\"\n",
    "    df[f\"{col}_num_contractions\"] = df[col].apply(\n",
    "        contraction_count\n",
    "    )  # num_contractions count\n",
    "\n",
    "    df[f\"{col}_polarity\"] = df[col].apply(\n",
    "        lambda x: TextBlob(x).sentiment[0]\n",
    "    )  # TextBlob 感情分析\n",
    "\n",
    "    df[f\"{col}_subjectivity\"] = df[col].apply(\n",
    "        lambda x: TextBlob(x).sentiment[1]\n",
    "    )  # TextBlob 感情分析\n",
    "\n",
    "    df[\n",
    "        [\n",
    "            f\"{col}_nn_count\",\n",
    "            f\"{col}_pr_count\",\n",
    "            f\"{col}_vb_count\",\n",
    "            f\"{col}_jj_count\",\n",
    "            f\"{col}_uh_count\",\n",
    "            f\"{col}_cd_count\",\n",
    "        ]\n",
    "    ] = df[col].apply(\n",
    "        pos_count\n",
    "    )  # pos count 品詞\n",
    "    \"\"\"\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_error(df, col):\n",
    "    df[\"GRAMMAR\"] = 0 #\n",
    "    df[\"TYPOS\"] = 0 #\n",
    "    df[\"TYPOGRAPHY\"] = 0 #\n",
    "    #df[\"REDUNDANCY\"] = 0\n",
    "    df[\"PUNCTUATION\"] = 0 #\n",
    "    #df[\"STYLE\"] = 0\n",
    "    #df[\"MISC\"] = 0\n",
    "    df[\"CASING\"] = 0 #\n",
    "    #df[\"CONFUSED_WORDS\"] = 0\n",
    "    #df[\"COLLOCATIONS\"] = 0\n",
    "    #df[\"NONSTANDARD_PHRASES\"] = 0\n",
    "    #df[\"BRITISH_ENGLISH\"] = 0\n",
    "    #df[\"SEMANTICS\"] = 0\n",
    "    #df[\"COMPOUNDING\"] = 0\n",
    "    #df[\"AMERICAN_ENGLISH_STYLE\"] = 0\n",
    "    df[\"cnt_error\"] = 0\n",
    "\n",
    "    use_category = [\"GRAMMAR\", \"TYPOS\", \"TYPOGRAPHY\", \"PUNCTUATION\", \"CASING\"]\n",
    "\n",
    "    for idx, text in enumerate(tqdm(df[col])):\n",
    "        for error in tl.check(text):\n",
    "            if error.category in use_category:\n",
    "                df.loc[idx, error.category] += 1\n",
    "            df.loc[idx, \"cnt_error\"] += 1\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import language_tool_python\n",
    "tl = language_tool_python.LanguageTool('en-US')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../../data/input/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = train[\"full_text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_spell(df, col):\n",
    "    spell = SpellChecker()\n",
    "    df[\"misspelled\"] = 0\n",
    "    for idx, text in enumerate(tqdm(df[col])):\n",
    "        text = re.split(\"[\\n\\n|\\r\\n|\\r|\\n| |!|#|$|%|&|(|)|*|+|,|-|.|/|:|;|<|=|>|?|@|\\|^|_|`|{|}|~|\\\"|\\||\\[|\\]]\", text)\n",
    "        misspelled = spell.unknown(text)\n",
    "        misspelled.remove(\"\")\n",
    "        df.loc[idx, \"misspelled\"] = len(misspelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[]] * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '\"a\" oh yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(\"[\\n\\n|\\r\\n|\\r|\\n| |!|#|$|%|&|(|)|*|+|,|-|.|/|:|;|<|=|>|?|@|\\|^|_|`|{|}|~|']\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str2 = re.split(\"[\\n\\n|\\r\\n|\\r|\\n| |!|#|$|%|&|(|)|*|+|,|-|.|/|:|;|<|=|>|?|@|\\|^|_|`|{|}|~|\\\"|\\||\\[|\\]]\", str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspelled = spell.unknown(str2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspelled.remove(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(misspelled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../../data/input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = text_features(train, \"full_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = text_features(test, \"full_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = check_error(train, \"full_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = check_error(test, \"full_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.iloc[:, -15:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.iloc[:, -15:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train[\"full_text\"]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train[\"full_text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.check(test)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {}\n",
    "a[\"b\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_check = {}\n",
    "cnt_check[\"GRAMMAR\"] = 0\n",
    "cnt_check[\"TYPOS\"] = 0\n",
    "cnt_check[\"TYPOGRAPHY\"] = 0\n",
    "cnt_check[\"REDUNDANCY\"] = 0\n",
    "cnt_check[\"PUNCTUATION\"] = 0\n",
    "cnt_check[\"STYLE\"] = 0\n",
    "cnt_check[\"MISC\"] = 0\n",
    "cnt_check[\"CASING\"] = 0\n",
    "cnt_check[\"CONFUSED_WORDS\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tl.check(test):\n",
    "    print(i.category)\n",
    "    cnt_check[i.category] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(textstat.automated_readability_index(tes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textstat.text_standard(tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tes = train[\"full_text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4.71 * (train[\"full_text_num_chars\"][10] / train[\"full_text_num_words\"][10]) + 0.5 * (train[\"full_text_num_words\"][10] / 20) -21.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(tes).split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = re.split(\"[\\n\\n|\\r\\n|\\r|\\n]\", tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tes.split(\". \"):\n",
    "    print(\"-----\")\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(\"\\n\\n|\\r\\n|\\r|\\n\", \" \", tes).replace(\"  \", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(\"\\n\\n|\\r\\n|\\r|\\n\", tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(filter(None, re.split(\"\\n\\n|\\r\\n|\\r|\\n\", tes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train[\"full_text\"][:10]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(data):\n",
    "    for i, t in enumerate(data[\"full_text\"]):\n",
    "        data.loc[i, \"full_text\"] = data.loc[i, \"full_text\"].replace(\"\\n\\n\", \"[BR]\")\n",
    "        data.loc[i, \"full_text\"] = data.loc[i, \"full_text\"].replace(\"\\r\\n\", \"[BR]\")\n",
    "        #data.loc[i, \"full_text\"] = data.loc[i, \"full_text\"].replace(\"\\n\", \"[BR]\")\n",
    "        #data.loc[i, \"full_text\"] = data.loc[i, \"full_text\"].replace(\"\\r\", \"[BR]\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(data):\n",
    "    for i, t in enumerate(data[\"full_text\"]):\n",
    "        data[\"full_text\"][i] = data[\"full_text\"][i].replace(\"\\n\\n\", \" \")\n",
    "        data[\"full_text\"][i] = data[\"full_text\"][i].replace(\"\\r\\n\", \" \")\n",
    "        data[\"full_text\"][i] = data[\"full_text\"][i].replace(\"\\n\", \" \")\n",
    "        data[\"full_text\"][i] = data[\"full_text\"][i].replace(\"\\r\", \" \")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_text(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"full_text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_tokens(\"[BR]\", special_tokens=True)\n",
    "#tokenizer.lf_token_id = tokenizer.get_added_vocab()['\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = tokenizer(train[\"full_text\"][0], add_special_tokens=True)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"[BR]\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"full_text\"][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in enumerate(train[\"full_text\"]):\n",
    "    if \"#\" in t:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"full_text\"][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for i, t in enumerate(train[\"full_text\"]):\n",
    "    if \"\\n\\n\" in t:\n",
    "        # print(i)\n",
    "        cnt += 1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for i, t in enumerate(train[\"full_text\"]):\n",
    "    if \"\" in t:\n",
    "        # print(i)\n",
    "        cnt += 1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3709 - 3667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"full_text\"][3852]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(text, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import sklearn.metrics as metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "class TableDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, cfg, X, y=None):\n",
    "        self.cfg = cfg\n",
    "        if y is None:\n",
    "            self.X = X.values\n",
    "            self.y = torch.zeros(len(self.X), dtype=torch.float32)\n",
    "        else:\n",
    "            self.X = X.values\n",
    "            self.y = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self._prepare_input(self.X[index])\n",
    "        y = self.y[index]\n",
    "        return X, y\n",
    "\n",
    "    def _prepare_input(self, X):\n",
    "        X = self.cfg[\"tokenizer\"].encode_plus(\n",
    "            X,\n",
    "            return_tensors=None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.cfg[\"tokenizer_params\"][\"max_length\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "        )\n",
    "        for k, v in X.items():\n",
    "            X[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(inputs):\n",
    "        # 一番長いtokenへ合わせる\n",
    "        mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = inputs[k][:, :mask_len]\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv\n",
    "test = pd.read_csv(\"../../data/input/test.csv\")\n",
    "test = test[\"full_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = { \"test_loader\":{\n",
    "    \"batch_size\": 4,\n",
    "    \"shuffle\": False,\n",
    "    #\"num_workers\": 2,\n",
    "    \"pin_memory\": True,\n",
    "    \"drop_last\": False\n",
    "    },\n",
    "    \"tokenizer_params\":{\n",
    "        \"max_length\": 512\n",
    "    }\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "cfg[\"tokenizer\"] = tokenizer\n",
    "test_dataset = TableDataset(cfg, test)\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    **cfg[\"test_loader\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, (inputs, labels) in enumerate(test_dataloader):\n",
    "    print(step)\n",
    "    print(inputs)\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_config = AutoConfig.from_pretrained(\n",
    "        \"microsoft/deberta-v3-base\", output_hidden_states=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained(\n",
    "                \"microsoft/deberta-v3-base\", config=tr_config\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [model.embeddings] + list(model.encoder.layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_decay = 0.5**(1./len(layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_decay**len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ｃ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = model.config.num_hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.95**13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.named_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debert = AutoModel.from_pretrained(\"microsoft/deberta-v3-base\", config=tr_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = debert(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([outputs[\"hidden_states\"][-1*i][:,0] for i in range(1, 4+1)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[\"hidden_states\"][0][:,0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in debert.named_parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"general\":{\n",
    "        \"seed\": 42,\n",
    "        \"cv\": True,\n",
    "        \"fold\": [0, 1, 2, 3, 4], # list (0-idx start) or null. set one element list, hold-out mode.\n",
    "        \"n_splits\": 5,\n",
    "        \"save_name\": \"submission_roberta\"\n",
    "    },\n",
    "\n",
    "    \"model\": \"transformers\",\n",
    "    \"model_name\": \"microsoft/deberta-v3-base\",\n",
    "    \"pretrained\": True,\n",
    "\n",
    "    \"header\": \"mean_pooling\",\n",
    "\n",
    "    \"tokenizer_params\":{\n",
    "        \"max_length\": 1429\n",
    "    },\n",
    "\n",
    "    \"transformers_params\":{\n",
    "        \"gradient_checkpointing\": True,\n",
    "        \"encoder_lr\": 2.0e-5,\n",
    "        \"decoder_lr\": 2.0e-5,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"num_cycles\": 0.5,\n",
    "        \"num_warmup_steps\": 0,\n",
    "        \"scheduler\": \"cosine\"\n",
    "    },\n",
    "\n",
    "    \"pl_params\":{\n",
    "        \"max_epochs\": 4,\n",
    "        \"accelerator\": \"auto\",\n",
    "        \"accumulate_grad_batches\": 1,\n",
    "        \"precision\": 16, # 16 or 32\n",
    "        \"deterministic\": False,\n",
    "        \"benchmark\": False,\n",
    "        \"enable_checkpointing\": False,\n",
    "        \"enable_model_summary\": False,\n",
    "        \"enable_progress_bar\": True,\n",
    "        \"logger\": False\n",
    "        # \"limit_train_batches\": 0.01, # for debug\n",
    "        # \"limit_val_batches\": 0.05, # for debug\n",
    "    },\n",
    "\n",
    "    \"early_stopping\": None,\n",
    "\n",
    "    \"criterion\": \"SmoothL1Loss\",\n",
    "\n",
    "    \"test_loader\":{\n",
    "        \"batch_size\": 4,\n",
    "        \"shuffle\": False,\n",
    "        \"num_workers\": 2,\n",
    "        \"pin_memory\": True,\n",
    "        \"drop_last\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = (\n",
    "            attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        )\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformersModel(pl.LightningModule):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.criterion = nn.__dict__[cfg[\"criterion\"]]()\n",
    "\n",
    "        self.tr_config = AutoConfig.from_pretrained(\n",
    "            cfg[\"model_name\"], output_hidden_states=True\n",
    "        )\n",
    "        self.tr_config.hidden_dropout = 0.0\n",
    "        self.tr_config.hidden_dropout_prob = 0.0\n",
    "        self.tr_config.attention_dropout = 0.0\n",
    "        self.tr_config.attention_probs_dropout_prob = 0.0\n",
    "        if cfg[\"pretrained\"]:\n",
    "            self.model = AutoModel.from_pretrained(\n",
    "                cfg[\"model_name\"], config=self.tr_config\n",
    "            )\n",
    "        else:\n",
    "            self.model = AutoModel(self.tr_config)\n",
    "        if self.cfg[\"transformers_params\"][\"gradient_checkpointing\"]:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "\n",
    "        # header\n",
    "        if cfg[\"header\"] == \"mean_pooling\":\n",
    "            self.pool = (MeanPooling(),)\n",
    "            self.fc = nn.Linear(self.tr_config.hidden_size, 6)\n",
    "        elif cfg[\"header\"] == \"cls_cancatenate\":\n",
    "            self.fc = (nn.Linear(self.tr_config.hidden_size * 4, 6),)\n",
    "\n",
    "        self._init_weights(self.fc)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.tr_config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.tr_config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        if self.cfg[\"header\"] == \"mean_pooling\":\n",
    "            last_hidden_states = outputs[0]\n",
    "            feature = self.pool(last_hidden_states, inputs[\"attention_mask\"])\n",
    "        elif self.cfg[\"header\"] == \"cls_cancatenate\":\n",
    "            feature = torch.cat(\n",
    "                [outputs[\"hidden_states\"][-1 * i][:, 0] for i in range(1, 4 + 1)], dim=1\n",
    "            )\n",
    "        return feature\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def collate(self, inputs):\n",
    "        # 一番長いtokenへ合わせる\n",
    "        mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = inputs[k][:, :mask_len]\n",
    "        return inputs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X = self.collate(X)\n",
    "        pred_y = self.forward(X).squeeze()\n",
    "        loss = self.criterion(pred_y, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        loss_list = [x[\"loss\"] for x in outputs]\n",
    "        avg_loss = torch.stack(loss_list).mean()\n",
    "        self.log(\"train_avg_loss\", avg_loss, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X = self.collate(X)\n",
    "        pred_y = self.forward(X).squeeze()\n",
    "        loss = self.criterion(pred_y, y)\n",
    "        self.log(\"valid_loss\", loss, prog_bar=True)\n",
    "        return {\"valid_loss\": loss, \"preds\": pred_y, \"targets\": y}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        loss_list = [x[\"valid_loss\"] for x in outputs]\n",
    "        preds = torch.cat([x[\"preds\"] for x in outputs], dim=0).cpu().detach().numpy()\n",
    "        targets = (\n",
    "            torch.cat([x[\"targets\"] for x in outputs], dim=0).cpu().detach().numpy()\n",
    "        )\n",
    "        avg_loss = torch.stack(loss_list).mean()\n",
    "        score, scores = mcrmse(targets, preds)\n",
    "        self.log(\"valid_avg_loss\", avg_loss, prog_bar=True)\n",
    "        self.log(\"valid_score\", score, prog_bar=True)\n",
    "        return avg_loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        X, _ = batch\n",
    "        X = self.collate(X)\n",
    "        pred_y = self.forward(X)\n",
    "        return pred_y\n",
    "\n",
    "    def get_scheduler(self, optimizer, num_train_steps):\n",
    "        if self.cfg[\"transformers_params\"][\"scheduler\"] == \"linear\":\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                num_warmup_steps=self.cfg[\"transformers_params\"][\"num_warmup_steps\"],\n",
    "                num_training_steps=num_train_steps,\n",
    "            )\n",
    "        elif self.cfg[\"transformers_params\"][\"scheduler\"] == \"cosine\":\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                num_warmup_steps=self.cfg[\"transformers_params\"][\"num_warmup_steps\"],\n",
    "                num_training_steps=num_train_steps,\n",
    "                num_cycles=self.cfg[\"transformers_params\"][\"num_cycles\"],\n",
    "            )\n",
    "        return scheduler\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.model.named_parameters()\n",
    "                    if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"lr\": self.cfg[\"transformers_params\"][\"encoder_lr\"],\n",
    "                \"weight_decay\": self.cfg[\"transformers_params\"][\"weight_decay\"],\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in self.model.named_parameters()\n",
    "                    if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"lr\": self.cfg[\"transformers_params\"][\"encoder_lr\"],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in self.named_parameters() if \"model\" not in n],\n",
    "                \"lr\": self.cfg[\"transformers_params\"][\"decoder_lr\"],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = optim.AdamW(\n",
    "            optimizer_parameters,\n",
    "            lr=self.cfg[\"transformers_params\"][\"encoder_lr\"],\n",
    "        )\n",
    "\n",
    "        scheduler = self.get_scheduler(\n",
    "            optimizer, self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformersModel(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in model.model.named_parameters():\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(outputs[\"hidden_states\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/input/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"../../data/input/train.csv\").drop([\"cohesion\", \"syntax\", \"vocabulary\", \"phraseology\", \"grammar\", \"conventions\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = df.loc[:, [\"cohesion\", \"syntax\", \"vocabulary\", \"phraseology\", \"grammar\", \"conventions\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[\"cohesion\"] = value[:, 0]\n",
    "df2[\"syntax\"] = value[:, 1]\n",
    "df2[\"vocabulary\"] = value[:, 2]\n",
    "df2[\"phraseology\"] = value[:, 3]\n",
    "df2[\"grammar\"] = value[:, 4]\n",
    "df2[\"conventions\"] = value[:, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"../../data/input/fb1_train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_list = list(pathlib.Path(input_dir).glob('**/*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(txt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_list_1 = txt_list[:3000]\n",
    "txt_list_2 = txt_list[3000:6000]\n",
    "txt_list_3 = txt_list[6000:9000]\n",
    "txt_list_4 = txt_list[9000:12000]\n",
    "txt_list_5 = txt_list[12000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(txt_list_1) + len(txt_list_2) + len(txt_list_3) + len(txt_list_4) + len(txt_list_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb1_train_text_id1 = []\n",
    "fb1_train_full_text1 = []\n",
    "for i in tqdm(txt_list_1):\n",
    "    with open(i, encoding=\"utf_8\") as f:\n",
    "        text_id = str(i).split(\"\\\\\")[-1][:-4]\n",
    "        fb1_train_text_id1.append(text_id)\n",
    "        fb1_train_full_text1.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "fb1_train_text_id2 = []\n",
    "fb1_train_full_text2 = []\n",
    "for i in tqdm(txt_list_2):\n",
    "    with open(i, encoding=\"utf_8\") as f:\n",
    "        text_id = str(i).split(\"\\\\\")[-1][:-4]\n",
    "        fb1_train_text_id2.append(text_id)\n",
    "        fb1_train_full_text2.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "fb1_train_text_id3 = []\n",
    "fb1_train_full_text3 = []\n",
    "for i in tqdm(txt_list_3):\n",
    "    with open(i, encoding=\"utf_8\") as f:\n",
    "        text_id = str(i).split(\"\\\\\")[-1][:-4]\n",
    "        fb1_train_text_id3.append(text_id)\n",
    "        fb1_train_full_text3.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "fb1_train_text_id4 = []\n",
    "fb1_train_full_text4 = []\n",
    "for i in tqdm(txt_list_4):\n",
    "    with open(i, encoding=\"utf_8\") as f:\n",
    "        text_id = str(i).split(\"\\\\\")[-1][:-4]\n",
    "        fb1_train_text_id4.append(text_id)\n",
    "        fb1_train_full_text4.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "fb1_train_text_id5 = []\n",
    "fb1_train_full_text5 = []\n",
    "for i in tqdm(txt_list_5):\n",
    "    with open(i, encoding=\"utf_8\") as f:\n",
    "        text_id = str(i).split(\"\\\\\")[-1][:-4]\n",
    "        fb1_train_text_id5.append(text_id)\n",
    "        fb1_train_full_text5.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fb1_train_text_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fb1_train_full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb2_train_df = pd.DataFrame(\n",
    "        data={\"text_id\": fb2_train_text_id, \"full_text\": fb2_train_full_text}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb2_train_df.to_csv(f\"../../data/input/fb2_train_.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb1 = pd.read_csv(f\"../../data/input/fb1_train_.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb1[\"fb\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb2 = pd.read_csv(f\"../../data/input/fb2_train_.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb2[\"fb\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb3 = pd.read_csv(f\"../../data/input/train.csv\").drop([\"cohesion\", \"syntax\", \"vocabulary\", \"phraseology\", \"grammar\", \"conventions\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb3[\"fb\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb = pd.concat([fb3, fb1, fb2]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_nodup = fb[~fb[\"text_id\"].duplicated()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_nodup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_nodup[fb_nodup[\"fb\"] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_nodup[fb_nodup[\"full_text\"].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('kaggle')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74b6f18d100cf4b6c08500594d0a08edb957b3f25d68b0d65c6705777eaf731f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
