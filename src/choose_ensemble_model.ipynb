{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T14:08:14.371555Z",
     "iopub.status.busy": "2022-11-28T14:08:14.370950Z",
     "iopub.status.idle": "2022-11-28T14:08:14.378847Z",
     "shell.execute_reply": "2022-11-28T14:08:14.378246Z",
     "shell.execute_reply.started": "2022-11-28T14:08:14.371490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../weights/deberta_v3_base_512_reinit_pseudo3/last_epoch_fold4.ckpt\n",
      "../../weights/deberta_v3_base_512_reinit_pseudo3/last_epoch_fold2.ckpt\n",
      "../../weights/deberta_v3_base_512_reinit_pseudo3/last_epoch_fold1.ckpt\n",
      "../../weights/deberta_v3_base_512_reinit_pseudo3/last_epoch_fold3.ckpt\n",
      "../../weights/deberta_v3_base_512_reinit_pseudo3/last_epoch_fold0.ckpt\n",
      "../../weights/deberta_v3_base_4096_reinit_pseudo/last_epoch_fold4.ckpt\n",
      "../../weights/deberta_v3_base_4096_reinit_pseudo/last_epoch_fold2.ckpt\n",
      "../../weights/deberta_v3_base_4096_reinit_pseudo/last_epoch_fold1.ckpt\n",
      "../../weights/deberta_v3_base_4096_reinit_pseudo/last_epoch_fold3.ckpt\n",
      "../../weights/deberta_v3_base_4096_reinit_pseudo/last_epoch_fold0.ckpt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk(\"../../weights\"):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYtAu4VWDoao"
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T14:08:14.382101Z",
     "iopub.status.busy": "2022-11-28T14:08:14.381924Z",
     "iopub.status.idle": "2022-11-28T14:08:18.613549Z",
     "shell.execute_reply": "2022-11-28T14:08:18.612861Z",
     "shell.execute_reply.started": "2022-11-28T14:08:14.382083Z"
    },
    "executionInfo": {
     "elapsed": 3718,
     "status": "ok",
     "timestamp": 1662355265196,
     "user": {
      "displayName": "Kento Morita",
      "userId": "02337995763048559377"
     },
     "user_tz": -540
    },
    "id": "Wy2A_ZTLDoaq"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "import sys\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import sklearn.metrics as metrics\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import yaml \n",
    "import joblib\n",
    "from sklearn import linear_model\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from scipy.optimize import minimize\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    get_cosine_schedule_with_warmup,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WMRIZRfDoZ3"
   },
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T14:08:18.615497Z",
     "iopub.status.busy": "2022-11-28T14:08:18.615070Z",
     "iopub.status.idle": "2022-11-28T14:08:18.694885Z",
     "shell.execute_reply": "2022-11-28T14:08:18.694293Z",
     "shell.execute_reply.started": "2022-11-28T14:08:18.615474Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"./configs/deberta_v3_base_512_reinit.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg1 = yaml.safe_load(f)\n",
    "cfg1[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg1[\"model_save\"] = False\n",
    "cfg1[\"enable_checkpointing\"] = False\n",
    "cfg1[\"pl_params\"][\"precision\"] = 32\n",
    "#cfg1[\"model_name\"] = \"../input/deberta-v3-base\"\n",
    "cfg1['save_weight_folder'] = \"../../weights/deberta_v3_base_512_reinit\"\n",
    "\n",
    "with open(\"./configs/deberta_v3_base_4096_reinit.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg2 = yaml.safe_load(f)\n",
    "cfg2[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg2[\"model_save\"] = False\n",
    "cfg2[\"enable_checkpointing\"] = False\n",
    "cfg2[\"pl_params\"][\"precision\"] = 32\n",
    "#cfg2[\"model_name\"] = \"../input/deberta-v3-base\"\n",
    "cfg2['save_weight_folder'] = \"../../weights/deberta_v3_base_4096_reinit\"\n",
    "\n",
    "with open(\"./configs/deberta_v3_large_512_reinit.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg3 = yaml.safe_load(f)\n",
    "cfg3[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg3[\"model_save\"] = False\n",
    "cfg3[\"enable_checkpointing\"] = False\n",
    "cfg3[\"pl_params\"][\"precision\"] = 32\n",
    "#cfg3[\"model_name\"] = \"../input/deberta-v3-large\"\n",
    "cfg3['save_weight_folder'] = \"../../weights/deberta_v3_large_512_reinit\"\n",
    "\n",
    "with open(\"./configs/deberta_v3_large_4096_reinit.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg4 = yaml.safe_load(f)\n",
    "cfg4[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg4[\"model_save\"] = False\n",
    "cfg4[\"enable_checkpointing\"] = False\n",
    "cfg4[\"pl_params\"][\"precision\"] = 32\n",
    "#cfg4[\"model_name\"] = \"../input/deberta-v3-large\"\n",
    "cfg4['save_weight_folder'] = \"../../weights/deberta_v3_large_4096_reinit\"\n",
    "\n",
    "with open(\"./configs/deberta_large_512_reinit.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg5 = yaml.safe_load(f)\n",
    "cfg5[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg5[\"model_save\"] = False\n",
    "cfg5[\"enable_checkpointing\"] = False\n",
    "cfg5[\"pl_params\"][\"precision\"] = 32\n",
    "#cfg5[\"model_name\"] = \"../input/deberta-large\"\n",
    "cfg5['save_weight_folder'] = \"../../weights/deberta_large_512_reinit\"\n",
    "\n",
    "with open(\"./configs/muppet_large_512_reinit.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg6 = yaml.safe_load(f)\n",
    "cfg6[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg6[\"model_save\"] = False\n",
    "cfg6[\"enable_checkpointing\"] = False\n",
    "cfg6[\"pl_params\"][\"precision\"] = 32\n",
    "#cfg6[\"model_name\"] = \"../input/muppet-roberta-large\"\n",
    "cfg6['save_weight_folder'] = \"../../weights/muppet_large_512_reinit\"\n",
    "\n",
    "with open(\"./configs/deberta_xlarge_512_reinit.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg7 = yaml.safe_load(f)\n",
    "cfg7[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg7[\"model_save\"] = False\n",
    "cfg7[\"enable_checkpointing\"] = False\n",
    "cfg7[\"pl_params\"][\"precision\"] = 32\n",
    "#cfg7[\"model_name\"] = \"../input/debertaxlarge\"\n",
    "cfg7['save_weight_folder'] = \"../../weights/deberta_xlarge_512_reinit\"\n",
    "\n",
    "with open(\"./configs/mpnet_base_512_reinit.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg8 = yaml.safe_load(f)\n",
    "cfg8[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg8[\"model_save\"] = False\n",
    "cfg8[\"enable_checkpointing\"] = False\n",
    "cfg8[\"pl_params\"][\"precision\"] = 32\n",
    "cfg8['save_weight_folder'] = \"../../weights/mpnet_base_512_reinit\"\n",
    "\n",
    "with open(\"./configs/deberta_v3_small_512_reinit.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg9 = yaml.safe_load(f)\n",
    "cfg9[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg9[\"model_save\"] = False\n",
    "cfg9[\"enable_checkpointing\"] = False\n",
    "cfg9[\"pl_params\"][\"precision\"] = 32\n",
    "cfg9['save_weight_folder'] = \"../../weights/deberta_v3_small_512_reinit\"\n",
    "\n",
    "with open(\"./configs/deberta_v3_xsmall_512_reinit.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg10 = yaml.safe_load(f)\n",
    "cfg10[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg10[\"model_save\"] = False\n",
    "cfg10[\"enable_checkpointing\"] = False\n",
    "cfg10[\"pl_params\"][\"precision\"] = 32\n",
    "cfg10['save_weight_folder'] = \"../../weights/deberta_v3_xsmall_512_reinit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T14:08:18.696226Z",
     "iopub.status.busy": "2022-11-28T14:08:18.696017Z",
     "iopub.status.idle": "2022-11-28T14:08:18.778180Z",
     "shell.execute_reply": "2022-11-28T14:08:18.777579Z",
     "shell.execute_reply.started": "2022-11-28T14:08:18.696207Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"./configs/deberta_v3_base_512_reinit_pseudo3.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg1p = yaml.safe_load(f)\n",
    "cfg1p[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg1p[\"model_save\"] = False\n",
    "cfg1p[\"enable_checkpointing\"] = False\n",
    "cfg1p[\"pl_params\"][\"precision\"] = 32\n",
    "#cfg1p[\"model_name\"] = \"../input/deberta-v3-base\"\n",
    "cfg1p['save_weight_folder'] = \"../../weights/deberta_v3_base_512_reinit_pseudo\"\n",
    "\n",
    "with open(\"./configs/deberta_v3_base_4096_reinit_pseudo.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg2p = yaml.safe_load(f)\n",
    "cfg2p[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg2p[\"model_save\"] = False\n",
    "cfg2p[\"enable_checkpointing\"] = False\n",
    "cfg2p[\"pl_params\"][\"precision\"] = 32\n",
    "#cfg2p[\"model_name\"] = \"../input/deberta-v3-base\"\n",
    "cfg2p['save_weight_folder'] = \"../../weights/deberta_v3_base_4096_reinit_pseudo\"\n",
    "\n",
    "with open(\"./configs/deberta_v3_large_512_reinit_pseudo3.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg3p = yaml.safe_load(f)\n",
    "cfg3p[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg3p[\"model_save\"] = False\n",
    "cfg3p[\"enable_checkpointing\"] = False\n",
    "cfg3p[\"pl_params\"][\"precision\"] = 32\n",
    "#cfg3p[\"model_name\"] = \"../input/deberta-v3-large\"\n",
    "cfg3p['save_weight_folder'] = \"../../weights/deberta_v3_large_512_reinit_pseudo\"\n",
    "\n",
    "with open(\"./configs/deberta_v3_large_4096_reinit_pseudo.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg4p = yaml.safe_load(f)\n",
    "cfg4p[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg4p[\"model_save\"] = False\n",
    "cfg4p[\"enable_checkpointing\"] = False\n",
    "cfg4p[\"pl_params\"][\"precision\"] = 32\n",
    "#cfg4p[\"model_name\"] = \"../input/deberta-v3-large\"\n",
    "cfg4p['save_weight_folder'] = \"../../weights/deberta_v3_large_4096_reinit_pseudo\"\n",
    "\n",
    "with open(\"./configs/deberta_large_512_reinit_pseudo.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg5p = yaml.safe_load(f)\n",
    "cfg5p[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg5p[\"model_save\"] = False\n",
    "cfg5p[\"enable_checkpointing\"] = False\n",
    "cfg5p[\"pl_params\"][\"precision\"] = 32\n",
    "#cfg5p[\"model_name\"] = \"../input/deberta-large\"\n",
    "cfg5p['save_weight_folder'] = \"../../weights/deberta_large_512_reinit_pseudo\"\n",
    "\n",
    "with open(\"./configs/muppet_large_512_reinit_pseudo.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg6p = yaml.safe_load(f)\n",
    "cfg6p[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg6p[\"model_save\"] = False\n",
    "cfg6p[\"enable_checkpointing\"] = False\n",
    "cfg6p[\"pl_params\"][\"precision\"] = 32\n",
    "#cfg6p[\"model_name\"] = \"../input/muppet-roberta-large\"\n",
    "cfg6p['save_weight_folder'] = \"../../weights/muppet_large_512_reinit_pseudo\"\n",
    "\n",
    "with open(\"./configs/deberta_xlarge_512_reinit_pseudo.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg7p = yaml.safe_load(f)\n",
    "cfg7p[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg7p[\"model_save\"] = False\n",
    "cfg7p[\"enable_checkpointing\"] = False\n",
    "cfg7p[\"pl_params\"][\"precision\"] = 32\n",
    "#cfg7p[\"model_name\"] = \"../input/debertaxlarge\"\n",
    "cfg7p['save_weight_folder'] = \"../../weights/deberta_xlarge_512_reinit_pseudo\"\n",
    "\n",
    "with open(\"./configs/deberta_v3_base_512_reinit_pseudo3_cls.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg8p = yaml.safe_load(f)\n",
    "cfg8p[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg8p[\"model_save\"] = False\n",
    "cfg8p[\"enable_checkpointing\"] = False\n",
    "cfg8p[\"pl_params\"][\"precision\"] = 32\n",
    "#cfg8p[\"model_name\"] = \"../input/deberta-v3-base\"\n",
    "cfg8p['save_weight_folder'] = \"../../weights/deberta_v3_base_512_reinit_pseudo3_cls\"\n",
    "\n",
    "with open(\"./configs/deberta_v3_base_512_reinit_pseudo3_wap_attention.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg9p = yaml.safe_load(f)\n",
    "cfg9p[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg9p[\"model_save\"] = False\n",
    "cfg9p[\"enable_checkpointing\"] = False\n",
    "cfg9p[\"pl_params\"][\"precision\"] = 32\n",
    "#cfg9p[\"model_name\"] = \"../input/deberta-v3-base\"\n",
    "cfg9p['save_weight_folder'] = \"../weights/deberta_v3_base_512_reinit_pseudo3_wap_attention\"\n",
    "\n",
    "with open(\"./configs/deberta_v3_base_512_reinit_pseudo3_wap_attention_3epoch.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg10p = yaml.safe_load(f)\n",
    "cfg10p[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg10p[\"model_save\"] = False\n",
    "cfg10p[\"enable_checkpointing\"] = False\n",
    "cfg10p[\"pl_params\"][\"precision\"] = 32\n",
    "#cfg10p[\"model_name\"] = \"../input/deberta-v3-base\"\n",
    "cfg10p['save_weight_folder'] = \"../weights/deberta_v3_base_512_reinit_pseudo3_wap_attention_3epoch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T14:08:18.779361Z",
     "iopub.status.busy": "2022-11-28T14:08:18.779155Z",
     "iopub.status.idle": "2022-11-28T14:08:18.887250Z",
     "shell.execute_reply": "2022-11-28T14:08:18.886732Z",
     "shell.execute_reply.started": "2022-11-28T14:08:18.779342Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"./configs/svr/albert_xxlarge_v2_512_svr.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg1s = yaml.safe_load(f)\n",
    "cfg1s[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg1s[\"model_save\"] = False\n",
    "cfg1s[\"enable_checkpointing\"] = False\n",
    "cfg1s[\"pl_params\"][\"precision\"] = 16\n",
    "#cfg1s[\"model_name\"] = \"../input/deberta-v3-base\"\n",
    "cfg1s['save_weight_folder'] = \"../../weights/albert_xxlarge_v2_512_svr\"\n",
    "\n",
    "with open(\"./configs/svr/bigbird_roberta_large_4096_svr.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg2s = yaml.safe_load(f)\n",
    "cfg2s[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg2s[\"model_save\"] = False\n",
    "cfg2s[\"enable_checkpointing\"] = False\n",
    "cfg2s[\"pl_params\"][\"precision\"] = 16\n",
    "#cfg2s[\"model_name\"] = \"../input/deberta-v3-base\"\n",
    "cfg2s['save_weight_folder'] = \"../../weights/bigbird_roberta_large_4096_svr\"\n",
    "\n",
    "with open(\"./configs/svr/deberta_large_512_svr.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg3s = yaml.safe_load(f)\n",
    "cfg3s[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg3s[\"model_save\"] = False\n",
    "cfg3s[\"enable_checkpointing\"] = False\n",
    "cfg3s[\"pl_params\"][\"precision\"] = 16\n",
    "#cfg3s[\"model_name\"] = \"../input/deberta-v3-base\"\n",
    "cfg3s['save_weight_folder'] = \"../../weights/deberta_large_512_svr\"\n",
    "\n",
    "with open(\"./configs/svr/deberta_v2_xlarge_512_svr.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg4s = yaml.safe_load(f)\n",
    "cfg4s[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg4s[\"model_save\"] = False\n",
    "cfg4s[\"enable_checkpointing\"] = False\n",
    "cfg4s[\"pl_params\"][\"precision\"] = 16\n",
    "#cfg4s[\"model_name\"] = \"../input/deberta-v3-base\"\n",
    "cfg4s['save_weight_folder'] = \"../../weights/deberta_v2_xlarge_512_svr\"\n",
    "\n",
    "with open(\"./configs/svr/deberta_v2_xxlarge_512_svr.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg5s = yaml.safe_load(f)\n",
    "cfg5s[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg5s[\"model_save\"] = False\n",
    "cfg5s[\"enable_checkpointing\"] = False\n",
    "cfg5s[\"pl_params\"][\"precision\"] = 16\n",
    "#cfg5s[\"model_name\"] = \"../input/deberta-v3-base\"\n",
    "cfg5s['save_weight_folder'] = \"../../weights/deberta_v2_xxlarge_512_svr\"\n",
    "\n",
    "with open(\"./configs/svr/deberta_v3_base_4096_svr.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg6s = yaml.safe_load(f)\n",
    "cfg6s[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg6s[\"model_save\"] = False\n",
    "cfg6s[\"enable_checkpointing\"] = False\n",
    "cfg6s[\"pl_params\"][\"precision\"] = 16\n",
    "#cfg6s[\"model_name\"] = \"../input/deberta-v3-base\"\n",
    "cfg6s['save_weight_folder'] = \"../../weights/deberta_v3_base_4096_svr\"\n",
    "\n",
    "with open(\"./configs/svr/deberta_v3_base_512_svr.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg7s = yaml.safe_load(f)\n",
    "cfg7s[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg7s[\"model_save\"] = False\n",
    "cfg7s[\"enable_checkpointing\"] = False\n",
    "cfg7s[\"pl_params\"][\"precision\"] = 16\n",
    "#cfg7s[\"model_name\"] = \"../input/deberta-v3-base\"\n",
    "cfg7s['save_weight_folder'] = \"../../weights/deberta_v3_base_512_svr\"\n",
    "\n",
    "with open(\"./configs/svr/deberta_v3_large_4096_svr.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg8s = yaml.safe_load(f)\n",
    "cfg8s[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg8s[\"model_save\"] = False\n",
    "cfg8s[\"enable_checkpointing\"] = False\n",
    "cfg8s[\"pl_params\"][\"precision\"] = 16\n",
    "#cfg8s[\"model_name\"] = \"../input/deberta-v3-base\"\n",
    "cfg8s['save_weight_folder'] = \"../../weights/deberta_v3_large_4096_svr\"\n",
    "\n",
    "with open(\"./configs/svr/deberta_v3_large_512_svr.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg9s = yaml.safe_load(f)\n",
    "cfg9s[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg9s[\"model_save\"] = False\n",
    "cfg9s[\"enable_checkpointing\"] = False\n",
    "cfg9s[\"pl_params\"][\"precision\"] = 16\n",
    "#cfg9s[\"model_name\"] = \"../input/deberta-v3-base\"\n",
    "cfg9s['save_weight_folder'] = \"../../weights/deberta_v3_large_512_svr\"\n",
    "\n",
    "with open(\"./configs/svr/deberta_xlarge_512_svr.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg10s = yaml.safe_load(f)\n",
    "cfg10s[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg10s[\"model_save\"] = False\n",
    "cfg10s[\"enable_checkpointing\"] = False\n",
    "cfg10s[\"pl_params\"][\"precision\"] = 16\n",
    "#cfg10s[\"model_name\"] = \"../input/deberta-v3-base\"\n",
    "cfg10s['save_weight_folder'] = \"../../weights/deberta_xlarge_512_svr\"\n",
    "\n",
    "with open(\"./configs/svr/electra_large_512_svr.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg11s = yaml.safe_load(f)\n",
    "cfg11s[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg11s[\"model_save\"] = False\n",
    "cfg11s[\"enable_checkpointing\"] = False\n",
    "cfg11s[\"pl_params\"][\"precision\"] = 16\n",
    "#cfg11s[\"model_name\"] = \"../input/deberta-v3-base\"\n",
    "cfg11s['save_weight_folder'] = \"../../weights/electra_large_512_svr\"\n",
    "\n",
    "with open(\"./configs/svr/funnel_xlarge_512_svr.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg12s = yaml.safe_load(f)\n",
    "cfg12s[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg12s[\"model_save\"] = False\n",
    "cfg12s[\"enable_checkpointing\"] = False\n",
    "cfg12s[\"pl_params\"][\"precision\"] = 16\n",
    "#cfg12s[\"model_name\"] = \"../input/deberta-v3-base\"\n",
    "cfg12s['save_weight_folder'] = \"../../weights/funnel_xlarge_512_svr\"\n",
    "\n",
    "with open(\"./configs/svr/longformer_large_4096.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg13s = yaml.safe_load(f)\n",
    "cfg13s[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg13s[\"model_save\"] = False\n",
    "cfg13s[\"enable_checkpointing\"] = False\n",
    "cfg13s[\"pl_params\"][\"precision\"] = 16\n",
    "#cfg13s[\"model_name\"] = \"../input/deberta-v3-base\"\n",
    "cfg13s['save_weight_folder'] = \"../../weights/longformer_large_4096\"\n",
    "\n",
    "with open(\"./configs/svr/muppet_large_512_svr.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg14s = yaml.safe_load(f)\n",
    "cfg14s[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg14s[\"model_save\"] = False\n",
    "cfg14s[\"enable_checkpointing\"] = False\n",
    "cfg14s[\"pl_params\"][\"precision\"] = 16\n",
    "#cfg14s[\"model_name\"] = \"../input/deberta-v3-base\"\n",
    "cfg14s['save_weight_folder'] = \"../../weights/muppet_large_512_svr\"\n",
    "\n",
    "with open(\"./configs/svr/roberta_large_512_svr.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg15s = yaml.safe_load(f)\n",
    "cfg15s[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg15s[\"model_save\"] = False\n",
    "cfg15s[\"enable_checkpointing\"] = False\n",
    "cfg15s[\"pl_params\"][\"precision\"] = 16\n",
    "#cfg15s[\"model_name\"] = \"../input/deberta-v3-base\"\n",
    "cfg15s['save_weight_folder'] = \"../../weights/roberta_large_512_svr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T14:11:58.500794Z",
     "iopub.status.busy": "2022-11-28T14:11:58.500235Z",
     "iopub.status.idle": "2022-11-28T14:11:58.833529Z",
     "shell.execute_reply": "2022-11-28T14:11:58.832793Z",
     "shell.execute_reply.started": "2022-11-28T14:11:58.500770Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"./configs/svr/deberta_v3_base_4096_reinit_svr_pseudo.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg1sp = yaml.safe_load(f)\n",
    "cfg1sp[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg1sp[\"model_save\"] = False\n",
    "cfg1sp[\"enable_checkpointing\"] = False\n",
    "cfg1sp[\"pl_params\"][\"precision\"] = 16\n",
    "#cfg1sp[\"model_name\"] = \"../input/deberta-v3-base\"\n",
    "cfg1sp['save_weight_folder'] = \"../../weights/deberta_v3_base_4096_reinit_svr_pseudo\"\n",
    "\n",
    "with open(\"./configs/svr/deberta_v3_base_512_reinit_svr_pseudo.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg2sp = yaml.safe_load(f)\n",
    "cfg2sp[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg2sp[\"model_save\"] = False\n",
    "cfg2sp[\"enable_checkpointing\"] = False\n",
    "cfg2sp[\"pl_params\"][\"precision\"] = 16\n",
    "#cfg2sp[\"model_name\"] = \"../input/deberta-v3-base\"\n",
    "cfg2sp['save_weight_folder'] = \"../../weights/deberta_v3_base_512_reinit_svr_pseudo\"\n",
    "\n",
    "with open(\"./configs/svr/deberta_v3_large_4096_reinit_svr_pseudo.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg3sp = yaml.safe_load(f)\n",
    "cfg3sp[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg3sp[\"model_save\"] = False\n",
    "cfg3sp[\"enable_checkpointing\"] = False\n",
    "cfg3sp[\"pl_params\"][\"precision\"] = 16\n",
    "#cfg3sp[\"model_name\"] = \"../input/deberta-v3-base\"\n",
    "cfg3sp['save_weight_folder'] = \"../../weights/deberta_v3_large_4096_reinit_svr_pseudo\"\n",
    "\n",
    "with open(\"./configs/svr/deberta_v3_large_512_reinit_svr_pseudo.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg4sp = yaml.safe_load(f)\n",
    "cfg4sp[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg4sp[\"model_save\"] = False\n",
    "cfg4sp[\"enable_checkpointing\"] = False\n",
    "cfg4sp[\"pl_params\"][\"precision\"] = 16\n",
    "#cfg4sp[\"model_name\"] = \"../input/deberta-v3-base\"\n",
    "cfg4sp['save_weight_folder'] = \"../../weights/deberta_v3_large_512_reinit_svr_pseudo\"\n",
    "\n",
    "with open(\"./configs/svr/deberta_xlarge_512_reinit_svr_pseudo.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg5sp = yaml.safe_load(f)\n",
    "cfg5sp[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg5sp[\"model_save\"] = False\n",
    "cfg5sp[\"enable_checkpointing\"] = False\n",
    "cfg5sp[\"pl_params\"][\"precision\"] = 16\n",
    "#cfg5sp[\"model_name\"] = \"../input/deberta-v3-base\"\n",
    "cfg5sp['save_weight_folder'] = \"../../weights/deberta_xlarge_512_reinit_svr_pseudo\"\n",
    "\n",
    "# 1128追加\n",
    "with open(\"./configs/svr/deberta_v3_base_512_reinit_svr_pseudo_wap.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg6sp = yaml.safe_load(f)\n",
    "cfg6sp[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg6sp[\"model_save\"] = False\n",
    "cfg6sp[\"enable_checkpointing\"] = False\n",
    "cfg6sp[\"pl_params\"][\"precision\"] = 16\n",
    "\n",
    "with open(\"./configs/svr/deberta_v3_base_512_reinit_svr_pseudo_wap_all.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg7sp = yaml.safe_load(f)\n",
    "cfg7sp[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg7sp[\"model_save\"] = False\n",
    "cfg7sp[\"enable_checkpointing\"] = False\n",
    "cfg7sp[\"pl_params\"][\"precision\"] = 16\n",
    "\n",
    "with open(\"./configs/svr/deberta_v3_base_512_reinit_svr_pseudo_mean_pooling_concatenate.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg8sp = yaml.safe_load(f)\n",
    "cfg8sp[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg8sp[\"model_save\"] = False\n",
    "cfg8sp[\"enable_checkpointing\"] = False\n",
    "cfg8sp[\"pl_params\"][\"precision\"] = 16\n",
    "\n",
    "with open(\"./configs/svr/deberta_v3_base_4096_reinit_svr_pseudo_wap.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg9sp = yaml.safe_load(f)\n",
    "cfg9sp[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg9sp[\"model_save\"] = False\n",
    "cfg9sp[\"enable_checkpointing\"] = False\n",
    "cfg9sp[\"pl_params\"][\"precision\"] = 16\n",
    "\n",
    "with open(\"./configs/svr/deberta_v3_base_4096_reinit_svr_pseudo_wap_all.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg10sp = yaml.safe_load(f)\n",
    "cfg10sp[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg10sp[\"model_save\"] = False\n",
    "cfg10sp[\"enable_checkpointing\"] = False\n",
    "cfg10sp[\"pl_params\"][\"precision\"] = 16\n",
    "\n",
    "with open(\"./configs/svr/deberta_v3_base_4096_reinit_svr_pseudo_mean_pooling_concatenate.yaml\", encoding=\"utf-8\") as f:\n",
    "    cfg11sp = yaml.safe_load(f)\n",
    "cfg11sp[\"general\"][\"wandb_desabled\"] = True\n",
    "cfg11sp[\"model_save\"] = False\n",
    "cfg11sp[\"enable_checkpointing\"] = False\n",
    "cfg11sp[\"pl_params\"][\"precision\"] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T14:08:18.934884Z",
     "iopub.status.busy": "2022-11-28T14:08:18.934176Z",
     "iopub.status.idle": "2022-11-28T14:08:18.937495Z",
     "shell.execute_reply": "2022-11-28T14:08:18.937077Z",
     "shell.execute_reply.started": "2022-11-28T14:08:18.934863Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg_list = [cfg1, cfg2, cfg3, cfg4, cfg5, cfg6, cfg7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T07:16:07.343327Z",
     "iopub.status.busy": "2022-11-28T07:16:07.343005Z",
     "iopub.status.idle": "2022-11-28T07:16:07.347228Z",
     "shell.execute_reply": "2022-11-28T07:16:07.346567Z",
     "shell.execute_reply.started": "2022-11-28T07:16:07.343304Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg_list_p = [cfg1p, cfg2p, cfg3p, cfg4p, cfg5p, cfg6p, cfg7p, cfg8p, cfg9p, cfg10p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T07:16:07.660634Z",
     "iopub.status.busy": "2022-11-28T07:16:07.659800Z",
     "iopub.status.idle": "2022-11-28T07:16:07.664513Z",
     "shell.execute_reply": "2022-11-28T07:16:07.663664Z",
     "shell.execute_reply.started": "2022-11-28T07:16:07.660600Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg_list_s = [cfg1s, cfg2s, cfg3s, cfg4s, cfg5s, cfg6s, cfg7s, cfg8s, cfg9s, cfg10s, cfg11s, cfg12s, cfg13s, cfg14s, cfg15s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T07:16:08.097354Z",
     "iopub.status.busy": "2022-11-28T07:16:08.096877Z",
     "iopub.status.idle": "2022-11-28T07:16:08.100976Z",
     "shell.execute_reply": "2022-11-28T07:16:08.100347Z",
     "shell.execute_reply.started": "2022-11-28T07:16:08.097326Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg_list_sp = [cfg1sp, cfg2sp, cfg3sp, cfg4sp, cfg5sp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T07:16:09.016557Z",
     "iopub.status.busy": "2022-11-28T07:16:09.015934Z",
     "iopub.status.idle": "2022-11-28T07:16:09.019783Z",
     "shell.execute_reply": "2022-11-28T07:16:09.019131Z",
     "shell.execute_reply.started": "2022-11-28T07:16:09.016533Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg_list_all = [cfg1, cfg2, cfg3, cfg4, cfg5, cfg6, cfg7] \\\n",
    "               [cfg1p, cfg2p, cfg3p, cfg4p, cfg5p, cfg6p, cfg7p, cfg8p, cfg9p, cfg10p] + \\\n",
    "               [cfg1s, cfg2s, cfg3s, cfg4s, cfg5s, cfg6s, cfg7s, cfg8s, cfg9s, cfg10s, cfg11s, cfg12s, cfg13s, cfg14s, cfg15s] + \\\n",
    "               [cfg1sp, cfg2sp, cfg3sp, cfg4sp, cfg5sp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T09:09:32.256047Z",
     "iopub.status.busy": "2022-11-28T09:09:32.255571Z",
     "iopub.status.idle": "2022-11-28T09:09:32.259850Z",
     "shell.execute_reply": "2022-11-28T09:09:32.259303Z",
     "shell.execute_reply.started": "2022-11-28T09:09:32.256022Z"
    }
   },
   "outputs": [],
   "source": [
    "#SVRなし最高\n",
    "cfg_list = [cfg2, cfg3, cfg4, cfg5, cfg6, cfg7, cfg1p, cfg2p, cfg3p, cfg4p, cfg7p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T13:37:18.742677Z",
     "iopub.status.busy": "2022-11-26T13:37:18.742415Z",
     "iopub.status.idle": "2022-11-26T13:37:18.748770Z",
     "shell.execute_reply": "2022-11-26T13:37:18.748281Z",
     "shell.execute_reply.started": "2022-11-26T13:37:18.742659Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg_list = [cfg1, cfg2, cfg3, cfg4, cfg5, cfg6, cfg7, cfg1p, cfg2p, cfg3p, cfg4p, cfg5p, cfg6p, cfg7p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T20:28:36.001241Z",
     "iopub.status.busy": "2022-11-26T20:28:36.000697Z",
     "iopub.status.idle": "2022-11-26T20:28:36.004523Z",
     "shell.execute_reply": "2022-11-26T20:28:36.003899Z",
     "shell.execute_reply.started": "2022-11-26T20:28:36.001215Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg_list = [cfg1, cfg2, cfg5, cfg6, cfg7, cfg1p, cfg3p, cfg4p, cfg7p, cfg2sp, cfg3sp, cfg4sp, cfg5sp] # 実験開始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T09:58:05.189000Z",
     "iopub.status.busy": "2022-11-26T09:58:05.188727Z",
     "iopub.status.idle": "2022-11-26T09:58:05.195222Z",
     "shell.execute_reply": "2022-11-26T09:58:05.194644Z",
     "shell.execute_reply.started": "2022-11-26T09:58:05.188981Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg_list = [cfg1, cfg2, cfg5, cfg6, cfg7, cfg1p, cfg3p, cfg4p, cfg7p] # <= subしたやつ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T14:32:28.674291Z",
     "iopub.status.busy": "2022-11-26T14:32:28.673789Z",
     "iopub.status.idle": "2022-11-26T14:32:28.677073Z",
     "shell.execute_reply": "2022-11-26T14:32:28.676677Z",
     "shell.execute_reply.started": "2022-11-26T14:32:28.674272Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg_list = [cfg1, cfg2, cfg5, cfg6, cfg7, cfg1p, cfg2p, cfg3p, cfg4p, cfg7p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-26T10:28:05.264084Z",
     "iopub.status.busy": "2022-11-26T10:28:05.263292Z",
     "iopub.status.idle": "2022-11-26T10:28:05.267067Z",
     "shell.execute_reply": "2022-11-26T10:28:05.266528Z",
     "shell.execute_reply.started": "2022-11-26T10:28:05.264066Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg_list = [cfg1, cfg2, cfg5, cfg6, cfg7, cfg1p, cfg3p, cfg4p, cfg7p, cfg1s, cfg2s, cfg3s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmES5GjMDoa6"
   },
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T14:10:01.986333Z",
     "iopub.status.busy": "2022-11-28T14:10:01.986056Z",
     "iopub.status.idle": "2022-11-28T14:10:02.032441Z",
     "shell.execute_reply": "2022-11-28T14:10:02.031774Z",
     "shell.execute_reply.started": "2022-11-28T14:10:01.986312Z"
    }
   },
   "outputs": [],
   "source": [
    "def mcrmse(y_trues, y_preds):\n",
    "    scores = []\n",
    "    idxes = y_trues.shape[1]\n",
    "    for i in range(idxes):\n",
    "        y_true = y_trues[:, i]\n",
    "        y_pred = y_preds[:, i]\n",
    "        score = metrics.mean_squared_error(y_true, y_pred, squared=False)  # RMSE\n",
    "        scores.append(score)\n",
    "    mcrmse_score = np.mean(scores)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = (\n",
    "            attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        )\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "\n",
    "class WeightedLayerPooling(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights=None):\n",
    "        super(WeightedLayerPooling, self).__init__()\n",
    "        self.layer_start = layer_start\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.layer_weights = (\n",
    "            layer_weights\n",
    "            if layer_weights is not None\n",
    "            else nn.Parameter(\n",
    "                torch.tensor(\n",
    "                    [1] * (num_hidden_layers + 1 - layer_start), dtype=torch.float\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, all_hidden_states):\n",
    "        all_layer_embedding = all_hidden_states[self.layer_start :, :, :, :]\n",
    "        weight_factor = (\n",
    "            self.layer_weights.unsqueeze(-1)\n",
    "            .unsqueeze(-1)\n",
    "            .unsqueeze(-1)\n",
    "            .expand(all_layer_embedding.size())\n",
    "        )\n",
    "        weighted_average = (weight_factor * all_layer_embedding).sum(\n",
    "            dim=0\n",
    "        ) / self.layer_weights.sum()\n",
    "        return weighted_average\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        self.att = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, last_hidden_state):\n",
    "        att_weights = self.att(last_hidden_state)\n",
    "        feature = torch.sum(att_weights * last_hidden_state, dim=1)\n",
    "        return feature\n",
    "\n",
    "\n",
    "class TransformersModel(pl.LightningModule):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.criterion = nn.__dict__[cfg[\"criterion\"]]()\n",
    "\n",
    "        # awp\n",
    "        if cfg[\"awp\"] is not None:\n",
    "            self.automatic_optimization = False\n",
    "            self.adv_param = cfg[\"awp\"][\"adv_param\"]\n",
    "            self.adv_lr = cfg[\"awp\"][\"adv_lr\"]\n",
    "            self.adv_eps = cfg[\"awp\"][\"adv_eps\"]\n",
    "            self.adv_step = cfg[\"awp\"][\"adv_step\"]\n",
    "            self.backup = {}\n",
    "            self.backup_eps = {}\n",
    "            self.awp_accumulate_grad_batches = cfg[\"awp\"][\"accumulate_grad_batches\"]\n",
    "            # self.awp_scaler = torch.cuda.amp.GradScaler(enabled=cfg[\"awp\"][\"amp\"])\n",
    "            if cfg[\"awp\"][\"gradient_clip_val\"] is not None:\n",
    "                self.awp_max_grad_norm = cfg[\"awp\"][\"gradient_clip_val\"]\n",
    "            else:\n",
    "                self.awp_max_grad_norm = None\n",
    "            self.awp_start_epoch = cfg[\"awp\"][\"start_epoch\"]\n",
    "\n",
    "        # model\n",
    "        if cfg[\"mlm\"]:\n",
    "            print(f\"../weights/mlm_model/{cfg['model_name']}\")\n",
    "            self.tr_config = AutoConfig.from_pretrained(\n",
    "                f\"../weights/mlm_model/{cfg['model_name']}\", output_hidden_states=True\n",
    "            )\n",
    "        else:\n",
    "            self.tr_config = AutoConfig.from_pretrained(\n",
    "                cfg[\"model_name\"], output_hidden_states=True\n",
    "            )\n",
    "\n",
    "        self.tr_config.hidden_dropout = 0.0\n",
    "        self.tr_config.hidden_dropout_prob = 0.0\n",
    "        self.tr_config.attention_dropout = 0.0\n",
    "        self.tr_config.attention_probs_dropout_prob = 0.0\n",
    "\n",
    "        if cfg[\"mlm\"]:\n",
    "            self.model = AutoModel.from_pretrained(\n",
    "                f\"../weights/mlm_model/{cfg['model_name']}\", config=self.tr_config\n",
    "            )\n",
    "        elif cfg[\"pretrained\"]:\n",
    "            self.model = AutoModel.from_pretrained(\n",
    "                cfg[\"model_name\"], config=self.tr_config\n",
    "            )\n",
    "        else:\n",
    "            self.model = AutoModel(self.tr_config)\n",
    "        if self.cfg[\"transformers_params\"][\"gradient_checkpointing\"]:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        if cfg[\"preprocess\"]:\n",
    "            self.model.resize_token_embeddings(len(cfg[\"tokenizer\"]))\n",
    "\n",
    "        # header\n",
    "        if cfg[\"header\"] == \"cls\":\n",
    "            self.fc = nn.Linear(self.tr_config.hidden_size, 6)\n",
    "            self._init_weights(self.fc)\n",
    "        elif cfg[\"header\"] == \"mean_pooling\":\n",
    "            self.pool = MeanPooling()\n",
    "            self.fc = nn.Linear(self.tr_config.hidden_size, 6)\n",
    "            self._init_weights(self.fc)\n",
    "        elif cfg[\"header\"] == \"attention\":\n",
    "            self.attention = AttentionHead(self.tr_config.hidden_size)\n",
    "            self.fc = nn.Linear(self.tr_config.hidden_size, 6)\n",
    "            self._init_weights(self.fc)\n",
    "        elif cfg[\"header\"] == \"weighted_average_pooling\":\n",
    "            layer_start = (\n",
    "                self.tr_config.num_hidden_layers + 1\n",
    "            ) - 4  # use last 4-layers\n",
    "            self.wl_pool = WeightedLayerPooling(\n",
    "                self.tr_config.num_hidden_layers,\n",
    "                layer_start=layer_start,\n",
    "                layer_weights=None,\n",
    "            )\n",
    "            self.m_pool = MeanPooling()\n",
    "            self.fc = nn.Linear(self.tr_config.hidden_size, 6)\n",
    "            self._init_weights(self.fc)\n",
    "        elif cfg[\"header\"] == \"cls_concatenate\":\n",
    "            self.fc = nn.Linear(self.tr_config.hidden_size * 4, 6)\n",
    "            self._init_weights(self.fc)\n",
    "        elif cfg[\"header\"] == \"mean_pooling_concatenate\":\n",
    "            self.pool = MeanPooling()\n",
    "            self.fc = nn.Linear(self.tr_config.hidden_size * 4, 6)\n",
    "            self._init_weights(self.fc)\n",
    "        elif cfg[\"header\"] == \"1dcnn\":\n",
    "            self.cnn1 = nn.Conv1d(\n",
    "                self.tr_config.hidden_size, 256, kernel_size=2, padding=1\n",
    "            )\n",
    "            self.cnn2 = nn.Conv1d(256, 6, kernel_size=2, padding=1)\n",
    "        elif cfg[\"header\"] == \"lstm\":\n",
    "            self.lstm = nn.LSTM(\n",
    "                self.tr_config.hidden_size,\n",
    "                self.tr_config.hidden_size,\n",
    "                batch_first=True,\n",
    "            )\n",
    "            self.fc = nn.Linear(self.tr_config.hidden_size, 6)\n",
    "            self._init_weights(self.fc)\n",
    "        elif cfg[\"header\"] == \"gru\":\n",
    "            self.gru = nn.GRU(\n",
    "                self.tr_config.hidden_size,\n",
    "                self.tr_config.hidden_size,\n",
    "                batch_first=True,\n",
    "            )\n",
    "            self.fc = nn.Linear(self.tr_config.hidden_size, 6)\n",
    "            self._init_weights(self.fc)\n",
    "\n",
    "        # reinit same layers\n",
    "        if cfg[\"transformers_params\"][\"reinit_layers\"] is not None:\n",
    "            print(f\"reinit {cfg['transformers_params']['reinit_layers']} layers\")\n",
    "            self._reinit_layer(self.model)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.tr_config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.tr_config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def _reinit_layer(self, model):\n",
    "        if \"funnel\" in self.cfg[\"model_name\"]:\n",
    "            for layer in model.decoder.layers[\n",
    "                -self.cfg[\"transformers_params\"][\"reinit_layers\"] :\n",
    "            ]:\n",
    "                for module in layer.modules():\n",
    "                    if isinstance(module, nn.Linear):\n",
    "                        module.weight.data.normal_(\n",
    "                            mean=0.0, std=model.config.initializer_range\n",
    "                        )\n",
    "                        if module.bias is not None:\n",
    "                            module.bias.data.zero_()\n",
    "                    elif isinstance(module, nn.Embedding):\n",
    "                        module.weight.data.normal_(\n",
    "                            mean=0.0, std=model.config.initializer_range\n",
    "                        )\n",
    "                        if module.padding_idx is not None:\n",
    "                            module.weight.data[module.padding_idx].zero_()\n",
    "                    elif isinstance(module, nn.LayerNorm):\n",
    "                        module.bias.data.zero_()\n",
    "                        module.weight.data.fill_(1.0)\n",
    "        else:\n",
    "            for layer in model.encoder.layer[\n",
    "                -self.cfg[\"transformers_params\"][\"reinit_layers\"] :\n",
    "            ]:\n",
    "                for module in layer.modules():\n",
    "                    if isinstance(module, nn.Linear):\n",
    "                        module.weight.data.normal_(\n",
    "                            mean=0.0, std=model.config.initializer_range\n",
    "                        )\n",
    "                        if module.bias is not None:\n",
    "                            module.bias.data.zero_()\n",
    "                    elif isinstance(module, nn.Embedding):\n",
    "                        module.weight.data.normal_(\n",
    "                            mean=0.0, std=model.config.initializer_range\n",
    "                        )\n",
    "                        if module.padding_idx is not None:\n",
    "                            module.weight.data[module.padding_idx].zero_()\n",
    "                    elif isinstance(module, nn.LayerNorm):\n",
    "                        module.bias.data.zero_()\n",
    "                        module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        if self.cfg[\"header\"] == \"cls\":\n",
    "            last_hidden_states = outputs[0][:, 0]\n",
    "            if self.cfg[\"retrieve_embeddings\"]:\n",
    "                outputs = F.normalize(outputs, p=2, dim=1)\n",
    "            else:\n",
    "                outputs = self.fc(last_hidden_states)\n",
    "        elif self.cfg[\"header\"] == \"mean_pooling\":\n",
    "            last_hidden_states = outputs[0]\n",
    "            feature = self.pool(last_hidden_states, inputs[\"attention_mask\"])\n",
    "            if self.cfg[\"retrieve_embeddings\"]:\n",
    "                outputs = F.normalize(feature, p=2, dim=1)\n",
    "            else:\n",
    "                outputs = self.fc(feature)\n",
    "        elif self.cfg[\"header\"] == \"attention\":\n",
    "            last_hidden_states = outputs[0]\n",
    "            feature = self.attention(last_hidden_states)\n",
    "            if self.cfg[\"retrieve_embeddings\"]:\n",
    "                outputs = F.normalize(feature, p=2, dim=1)\n",
    "            else:\n",
    "                outputs = self.fc(feature)\n",
    "        elif self.cfg[\"header\"] == \"weighted_average_pooling\":\n",
    "            all_hidden_states = torch.stack(outputs[\"hidden_states\"])\n",
    "            weighted_pooling_embeddings = self.wl_pool(all_hidden_states)\n",
    "            feature = self.m_pool(weighted_pooling_embeddings, inputs[\"attention_mask\"])\n",
    "            if self.cfg[\"retrieve_embeddings\"]:\n",
    "                outputs = F.normalize(feature, p=2, dim=1)\n",
    "            else:\n",
    "                outputs = self.fc(feature)\n",
    "        elif self.cfg[\"header\"] == \"cls_concatenate\":\n",
    "            feature = torch.cat(\n",
    "                [outputs[\"hidden_states\"][-1 * i][:, 0] for i in range(1, 4 + 1)], dim=1\n",
    "            )\n",
    "            if self.cfg[\"retrieve_embeddings\"]:\n",
    "                outputs = F.normalize(feature, p=2, dim=1)\n",
    "            else:\n",
    "                outputs = self.fc(feature)\n",
    "        if self.cfg[\"header\"] == \"mean_pooling_concatenate\":\n",
    "            features = []\n",
    "            for i in range(1, 4 + 1):\n",
    "                last_hidden_states = outputs[\"hidden_states\"][-1 * i]\n",
    "                feature = self.pool(last_hidden_states, inputs[\"attention_mask\"])\n",
    "                features.append(feature)\n",
    "            feature = torch.cat(features, dim=1)\n",
    "            if self.cfg[\"retrieve_embeddings\"]:\n",
    "                outputs = F.normalize(feature, p=2, dim=1)\n",
    "            else:\n",
    "                outputs = self.fc(feature)\n",
    "        elif self.cfg[\"header\"] == \"1d_cnn\":\n",
    "            last_hidden_states = outputs[\"last_hidden_state\"].permute(0, 2, 1)\n",
    "            if self.cfg[\"retrieve_embeddings\"]:\n",
    "                outputs = F.normalize(last_hidden_states, p=2, dim=1)\n",
    "            else:\n",
    "                cnn_embeddings = F.relu(self.cnn1(last_hidden_states))\n",
    "                cnn_embeddings = self.cnn2(cnn_embeddings)\n",
    "                outputs, _ = torch.max(cnn_embeddings, 2)\n",
    "        elif self.cfg[\"header\"] == \"lstm\":\n",
    "            feature, _ = self.lstm(outputs[\"last_hidden_state\"], None)\n",
    "            feature = feature[:, -1, :]\n",
    "            if self.cfg[\"retrieve_embeddings\"]:\n",
    "                outputs = F.normalize(feature, p=2, dim=1)\n",
    "            else:\n",
    "                outputs = self.fc(feature)\n",
    "        elif self.cfg[\"header\"] == \"gru\":\n",
    "            feature, _ = self.gru(outputs[\"last_hidden_state\"], None)\n",
    "            feature = feature[:, -1, :]\n",
    "            if self.cfg[\"retrieve_embeddings\"]:\n",
    "                outputs = F.normalize(feature, p=2, dim=1)\n",
    "            else:\n",
    "                outputs = self.fc(feature)\n",
    "        return outputs\n",
    "\n",
    "    def collate(self, inputs):\n",
    "        # 一番長いtokenへ合わせる\n",
    "        mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = inputs[k][:, :mask_len]\n",
    "        return inputs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X = self.collate(X)\n",
    "        if self.cfg[\"awp\"] is not None:\n",
    "            # awp step\n",
    "            opt = self.optimizers()\n",
    "            sch = self.lr_schedulers()\n",
    "\n",
    "            # with torch.cuda.amp.autocast(enabled=self.cfg[\"awp\"][\"amp\"]):\n",
    "            pred_y = self.forward(X)\n",
    "            loss = self.criterion(pred_y, y)\n",
    "\n",
    "            if self.awp_accumulate_grad_batches > 1:\n",
    "                loss = loss / self.awp_accumulate_grad_batches\n",
    "            # self.awp_scaler.scale(loss).backward()\n",
    "            self.manual_backward(loss)\n",
    "\n",
    "            if (batch_idx + 1) % self.awp_accumulate_grad_batches == 0:\n",
    "                if self.trainer.current_epoch >= self.awp_start_epoch:\n",
    "                    self._awp_save()\n",
    "                    for _ in range(self.adv_step):\n",
    "                        self._awp_attack_step()\n",
    "                        # with torch.cuda.amp.autocast(enabled=self.cfg[\"awp\"][\"amp\"]):\n",
    "                        pred_y = self.forward(X)\n",
    "                        adv_loss = self.criterion(pred_y, y)\n",
    "                        opt.zero_grad()\n",
    "                        # self.awp_scaler.scale(adv_loss).backward()\n",
    "                        self.manual_backward(adv_loss)\n",
    "                    self._awp_restore()\n",
    "\n",
    "                # self.awp_scaler.unscale_(opt)\n",
    "                # torch.nn.utils.clip_grad_norm_(\n",
    "                #    self.parameters(), self.awp_max_grad_norm\n",
    "                # )\n",
    "                # self.awp_scaler.step(opt)\n",
    "                opt.step()\n",
    "                # self.awp_scaler.update()\n",
    "                opt.zero_grad()\n",
    "                sch.step()\n",
    "        else:\n",
    "            # normal step\n",
    "            pred_y = self.forward(X)\n",
    "            loss = self.criterion(pred_y, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        loss_list = [x[\"loss\"] for x in outputs]\n",
    "        avg_loss = torch.stack(loss_list).mean()\n",
    "        self.log(\"train_avg_loss\", avg_loss, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X = self.collate(X)\n",
    "        # if self.cfg[\"awp\"] is not None:\n",
    "        #    with torch.cuda.amp.autocast(enabled=self.cfg[\"awp\"][\"amp\"]):\n",
    "        #        pred_y = self.forward(X)\n",
    "        #        loss = self.criterion(pred_y, y)\n",
    "        # else:\n",
    "        pred_y = self.forward(X)\n",
    "        loss = self.criterion(pred_y, y)\n",
    "        return {\"valid_loss\": loss, \"preds\": pred_y, \"targets\": y}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        loss_list = [x[\"valid_loss\"] for x in outputs]\n",
    "        preds = torch.cat([x[\"preds\"] for x in outputs], dim=0).cpu().detach().numpy()\n",
    "        targets = (\n",
    "            torch.cat([x[\"targets\"] for x in outputs], dim=0).cpu().detach().numpy()\n",
    "        )\n",
    "        avg_loss = torch.stack(loss_list).mean()\n",
    "        score, scores = mcrmse(targets, preds)\n",
    "        self.log(\"valid_avg_loss\", avg_loss, prog_bar=True)\n",
    "        self.log(\"valid_score\", score, prog_bar=True)\n",
    "        self.log(\"valid_cohesion\", scores[0], prog_bar=True)\n",
    "        self.log(\"valid_syntax\", scores[1], prog_bar=True)\n",
    "        self.log(\"valid_vocabulary\", scores[2], prog_bar=True)\n",
    "        self.log(\"valid_phraseology\t\", scores[3], prog_bar=True)\n",
    "        self.log(\"valid_grammar\", scores[4], prog_bar=True)\n",
    "        self.log(\"valid_conventions\", scores[5], prog_bar=True)\n",
    "        return avg_loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        X, _ = batch\n",
    "        X = self.collate(X)\n",
    "        pred_y = self.forward(X)\n",
    "        return pred_y\n",
    "\n",
    "    def get_scheduler(self, optimizer, num_train_steps):\n",
    "        if self.cfg[\"transformers_params\"][\"scheduler\"] == \"linear\":\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                num_warmup_steps=self.cfg[\"transformers_params\"][\"warmup_ratio\"]\n",
    "                * num_train_steps,\n",
    "                num_training_steps=num_train_steps,\n",
    "            )\n",
    "        elif self.cfg[\"transformers_params\"][\"scheduler\"] == \"cosine\":\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                num_warmup_steps=self.cfg[\"transformers_params\"][\"warmup_ratio\"]\n",
    "                * num_train_steps,\n",
    "                num_training_steps=num_train_steps,\n",
    "                num_cycles=self.cfg[\"transformers_params\"][\"num_cycles\"],\n",
    "            )\n",
    "        return scheduler\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # ↓ decayする層を選択 & header (decoder) には別の学習率を設定\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in self.named_parameters() if \"model\" not in n],\n",
    "                \"lr\": self.cfg[\"transformers_params\"][\"decoder_lr\"],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        if \"funnel\" in self.cfg[\"model_name\"]:\n",
    "            layers = (\n",
    "                [self.model.embeddings]\n",
    "                + list(self.model.encoder.blocks)\n",
    "                + list(self.model.decoder.layers)\n",
    "            )\n",
    "        else:\n",
    "            layers = [self.model.embeddings] + list(self.model.encoder.layer)\n",
    "        layers.reverse()\n",
    "        lr = self.cfg[\"transformers_params\"][\"encoder_lr\"]\n",
    "        lr_decay = self.cfg[\"transformers_params\"][\"lr_decay_final\"] ** (\n",
    "            1.0 / len(layers)\n",
    "        )\n",
    "        for layer in layers:\n",
    "            optimizer_parameters += [\n",
    "                {\n",
    "                    \"params\": [\n",
    "                        p\n",
    "                        for n, p in layer.named_parameters()\n",
    "                        if not any(nd in n for nd in no_decay)\n",
    "                    ],\n",
    "                    \"weight_decay\": self.cfg[\"transformers_params\"][\"weight_decay\"],\n",
    "                    \"lr\": lr,\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [\n",
    "                        p\n",
    "                        for n, p in layer.named_parameters()\n",
    "                        if any(nd in n for nd in no_decay)\n",
    "                    ],\n",
    "                    \"weight_decay\": 0.0,\n",
    "                    \"lr\": lr,\n",
    "                },\n",
    "            ]\n",
    "            lr *= lr_decay\n",
    "\n",
    "        optimizer = optim.AdamW(\n",
    "            optimizer_parameters, lr=self.cfg[\"transformers_params\"][\"encoder_lr\"],\n",
    "        )\n",
    "\n",
    "        if self.cfg[\"awp\"] is None:\n",
    "            num_train_steps = self.trainer.estimated_stepping_batches\n",
    "        else:\n",
    "            num_train_steps = (\n",
    "                self.trainer.estimated_stepping_batches\n",
    "                / self.awp_accumulate_grad_batches\n",
    "            )\n",
    "\n",
    "        scheduler = self.get_scheduler(optimizer, num_train_steps)\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    ###############################################################################\n",
    "    # awp -------------------------------------------------------------------------\n",
    "    ###############################################################################\n",
    "    def on_before_optimizer_step(self, optimizer, optimizer_idx):\n",
    "        if self.cfg[\"awp\"] is not None:\n",
    "            self.clip_gradients(\n",
    "                optimizer,\n",
    "                gradient_clip_val=self.awp_max_grad_norm,\n",
    "                gradient_clip_algorithm=None,\n",
    "            )\n",
    "\n",
    "    def _awp_attack_step(self):\n",
    "        e = 1e-6\n",
    "        for name, param in self.named_parameters():\n",
    "            if (\n",
    "                param.requires_grad\n",
    "                and param.grad is not None\n",
    "                and self.adv_param in name\n",
    "            ):\n",
    "                norm1 = torch.norm(param.grad)\n",
    "                norm2 = torch.norm(param.data.detach())\n",
    "                if norm1 != 0 and not torch.isnan(norm1):\n",
    "                    r_at = self.adv_lr * param.grad / (norm1 + e) * (norm2 + e)\n",
    "                    param.data.add_(r_at)\n",
    "                    param.data = torch.min(\n",
    "                        torch.max(param.data, self.backup_eps[name][0]),\n",
    "                        self.backup_eps[name][1],\n",
    "                    )\n",
    "                # param.data.clamp_(*self.backup_eps[name])\n",
    "\n",
    "    def _awp_save(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if (\n",
    "                param.requires_grad\n",
    "                and param.grad is not None\n",
    "                and self.adv_param in name\n",
    "            ):\n",
    "                if name not in self.backup:\n",
    "                    self.backup[name] = param.data.clone()\n",
    "                    grad_eps = self.adv_eps * param.abs().detach()\n",
    "                    self.backup_eps[name] = (\n",
    "                        self.backup[name] - grad_eps,\n",
    "                        self.backup[name] + grad_eps,\n",
    "                    )\n",
    "\n",
    "    def _awp_restore(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if name in self.backup:\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "        self.backup_eps = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T14:10:02.187423Z",
     "iopub.status.busy": "2022-11-28T14:10:02.186568Z",
     "iopub.status.idle": "2022-11-28T14:10:02.195579Z",
     "shell.execute_reply": "2022-11-28T14:10:02.194889Z",
     "shell.execute_reply.started": "2022-11-28T14:10:02.187394Z"
    },
    "executionInfo": {
     "elapsed": 50,
     "status": "ok",
     "timestamp": 1662355265211,
     "user": {
      "displayName": "Kento Morita",
      "userId": "02337995763048559377"
     },
     "user_tz": -540
    },
    "id": "ISDPW054DobK"
   },
   "outputs": [],
   "source": [
    "class TableDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, cfg, X, y=None):\n",
    "        self.cfg = cfg\n",
    "        if y is None:\n",
    "            self.X = X.values\n",
    "            self.y = torch.zeros(len(self.X), dtype=torch.float32)\n",
    "        else:\n",
    "            self.X = X.values\n",
    "            self.y = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self._prepare_input(self.X[index])\n",
    "        y = self.y[index]\n",
    "        return X, y\n",
    "\n",
    "    def _prepare_input(self, X):\n",
    "        if self.cfg[\"token_cut_head_and_tail\"]:\n",
    "            X = self.cut_head_and_tail(X)\n",
    "        else:\n",
    "            X = self.cfg[\"tokenizer\"].encode_plus(\n",
    "                X,\n",
    "                return_tensors=None,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.cfg[\"tokenizer_params\"][\"max_length\"],\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "            )\n",
    "        for k, v in X.items():\n",
    "            X[k] = torch.tensor(v, dtype=torch.long)\n",
    "        return X\n",
    "\n",
    "    def cut_head_and_tail(self, text):\n",
    "        # まずは限界を設定せずにトークナイズする\n",
    "        max_len = self.cfg[\"tokenizer_params\"][\"max_length\"]\n",
    "        input_ids = self.cfg[\"tokenizer\"].encode(text)\n",
    "        n_token = len(input_ids)\n",
    "\n",
    "        # トークン数が最大数と同じ場合\n",
    "        if n_token == max_len:\n",
    "            input_ids = input_ids\n",
    "            attention_mask = [1 for _ in range(max_len)]\n",
    "            token_type_ids = [1 for _ in range(max_len)]\n",
    "        # トークン数が最大数より少ない場合\n",
    "        elif n_token < max_len:\n",
    "            pad = [1 for _ in range(max_len - n_token)]\n",
    "            input_ids = input_ids + pad\n",
    "            attention_mask = [1 if n_token > i else 0 for i in range(max_len)]\n",
    "            token_type_ids = [1 if n_token > i else 0 for i in range(max_len)]\n",
    "        # トークン数が最大数より多い場合\n",
    "        else:\n",
    "            harf_len = (max_len - 2) // 2\n",
    "            _input_ids = input_ids[1:-1]\n",
    "            input_ids = [0] + _input_ids[:harf_len] + _input_ids[-harf_len:] + [2]\n",
    "            attention_mask = [1 for _ in range(max_len)]\n",
    "            token_type_ids = [1 for _ in range(max_len)]\n",
    "\n",
    "        d = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "        }\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T14:10:03.191234Z",
     "iopub.status.busy": "2022-11-28T14:10:03.190619Z",
     "iopub.status.idle": "2022-11-28T14:10:03.196674Z",
     "shell.execute_reply": "2022-11-28T14:10:03.195992Z",
     "shell.execute_reply.started": "2022-11-28T14:10:03.191208Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformersRegressorInference:\n",
    "    def __init__(self, cfg, weight_path=None):\n",
    "        # tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(cfg[\"model_name\"])\n",
    "        if cfg[\"preprocess\"]:\n",
    "            tokenizer.add_tokens(\"[BR]\", special_tokens=True)\n",
    "        cfg[\"tokenizer\"] = tokenizer\n",
    "\n",
    "        self.model = TransformersModel(cfg)\n",
    "        self.weight_path = weight_path\n",
    "        self.cfg = cfg\n",
    "        self.trainer = Trainer(**self.cfg[\"pl_params\"])\n",
    "\n",
    "    def predict(self, test_X):\n",
    "        preds = []\n",
    "        test_dataset = TableDataset(self.cfg, test_X)\n",
    "        test_dataloader = torch.utils.data.DataLoader(\n",
    "            test_dataset, **self.cfg[\"test_loader\"],\n",
    "        )\n",
    "        preds = self.trainer.predict(\n",
    "            self.model, dataloaders=test_dataloader, ckpt_path=self.weight_path\n",
    "        )\n",
    "        preds = torch.cat(preds, axis=0)\n",
    "        preds = preds.cpu().detach().numpy()\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxkQx-RFDobM"
   },
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T14:10:48.155910Z",
     "iopub.status.busy": "2022-11-28T14:10:48.155608Z",
     "iopub.status.idle": "2022-11-28T14:10:48.312891Z",
     "shell.execute_reply": "2022-11-28T14:10:48.312181Z",
     "shell.execute_reply.started": "2022-11-28T14:10:48.155888Z"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1662355266221,
     "user": {
      "displayName": "Kento Morita",
      "userId": "02337995763048559377"
     },
     "user_tz": -540
    },
    "id": "sqwyL7gnDobN"
   },
   "outputs": [],
   "source": [
    "# read csv\n",
    "train = pd.read_csv(\"../data/input/train.csv\")\n",
    "test = pd.read_csv(\"../data/input/test.csv\")\n",
    "sub = pd.read_csv(\"../data/input/sample_submission.csv\")\n",
    "\n",
    "# split X/y\n",
    "train_X = train[\"full_text\"]\n",
    "train_y = train.drop([\"text_id\", \"full_text\"], axis=1)\n",
    "test_X = test[\"full_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PevYBVQDobO"
   },
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T14:10:49.834654Z",
     "iopub.status.busy": "2022-11-28T14:10:49.833877Z",
     "iopub.status.idle": "2022-11-28T14:10:49.838415Z",
     "shell.execute_reply": "2022-11-28T14:10:49.837824Z",
     "shell.execute_reply.started": "2022-11-28T14:10:49.834629Z"
    }
   },
   "outputs": [],
   "source": [
    "def one_fold_valid(skf, cfg, valid_X, valid_y, fold_n):\n",
    "    #print(f\"[fold_{fold_n}]\")\n",
    "    #seed_everything(cfg[\"general\"][\"seed\"], workers=True)\n",
    "\n",
    "    if use_computed:\n",
    "        valid_preds = joblib.load(f\"../data/preds/valid_{cfg['general']['seed']}_{cfg['general']['save_name']}_{fold_n}.preds\")\n",
    "    else:\n",
    "        model = TransformersRegressorInference(cfg, f\"{cfg['save_weight_folder']}/last_epoch_fold{fold_n}.ckpt\")\n",
    "        valid_preds = model.predict(valid_X)\n",
    "        \n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    #score, scores = mcrmse(valid_y.values, valid_preds)\n",
    "    #print(f\"mcrmse_score:{score}, mcrmse_scores:{scores}\")\n",
    "\n",
    "    return valid_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T14:10:50.574574Z",
     "iopub.status.busy": "2022-11-28T14:10:50.573985Z",
     "iopub.status.idle": "2022-11-28T14:10:50.584644Z",
     "shell.execute_reply": "2022-11-28T14:10:50.583877Z",
     "shell.execute_reply.started": "2022-11-28T14:10:50.574544Z"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1662355265201,
     "user": {
      "displayName": "Kento Morita",
      "userId": "02337995763048559377"
     },
     "user_tz": -540
    },
    "id": "R_-bDv4rDoau",
    "outputId": "e5532017-d235-4faa-d869-38e38fe4d3d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random seed setting\n",
    "seed_everything(cfg1[\"general\"][\"seed\"], workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T14:10:51.220094Z",
     "iopub.status.busy": "2022-11-28T14:10:51.219502Z",
     "iopub.status.idle": "2022-11-28T14:10:51.223259Z",
     "shell.execute_reply": "2022-11-28T14:10:51.222557Z",
     "shell.execute_reply.started": "2022-11-28T14:10:51.220067Z"
    }
   },
   "outputs": [],
   "source": [
    "use_computed = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimize and blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T14:10:53.203472Z",
     "iopub.status.busy": "2022-11-28T14:10:53.203186Z",
     "iopub.status.idle": "2022-11-28T14:10:53.215247Z",
     "shell.execute_reply": "2022-11-28T14:10:53.214553Z",
     "shell.execute_reply.started": "2022-11-28T14:10:53.203450Z"
    }
   },
   "outputs": [],
   "source": [
    "def choose(cfg_list):\n",
    "    fold_list = range(cfg1[\"general\"][\"n_splits\"])\n",
    "    preds_list_cv = [[] for i in range(cfg1[\"general\"][\"n_splits\"])]\n",
    "    weights_list = [[] for _ in range(6)]\n",
    "    skf = MultilabelStratifiedKFold(\n",
    "        n_splits=cfg1[\"general\"][\"n_splits\"], shuffle=True, random_state=cfg1[\"general\"][\"seed\"]\n",
    "    )\n",
    "    for j, fold_n in enumerate(fold_list):\n",
    "        preds_list = []\n",
    "        _, valid_indices = list(skf.split(train_X, train_y))[fold_n]\n",
    "        valid_X_cv, valid_y_cv = (\n",
    "            train_X.iloc[valid_indices].reset_index(drop=True),\n",
    "            train_y.iloc[valid_indices].reset_index(drop=True),\n",
    "        )\n",
    "\n",
    "        for i, cfg in enumerate(cfg_list):\n",
    "            valid_preds = one_fold_valid(skf, cfg, valid_X_cv, valid_y_cv, fold_n)\n",
    "            preds_list.append(valid_preds)\n",
    "            #del valid_preds\n",
    "            #gc.collect()\n",
    "\n",
    "        #score, scores = mcrmse(valid_y_cv.values, np.mean(preds_list, axis=0))\n",
    "        #print()\n",
    "        #print(f\"simple_mean: mcrmse_score:{score}, mcrmse_scores:{scores}\")\n",
    "        #for i, p in enumerate(preds_list):\n",
    "            #score, scores = mcrmse(valid_y_cv.values, p)\n",
    "            #print(f\"cfg_{i}: mcrmse_score:{score}, mcrmse_scores:{scores}\")\n",
    "\n",
    "        preds_list = np.array(preds_list)\n",
    "        preds_list_cv[j] = preds_list\n",
    "        cohesion = preds_list[:, :, 0]\n",
    "        syntax = preds_list[:, :, 1]\n",
    "        vocabulary = preds_list[:, :, 2]\n",
    "        phraseology = preds_list[:, :, 3]\n",
    "        grammar = preds_list[:, :, 4]\n",
    "        conventions = preds_list[:, :, 5]\n",
    "        target_list = [cohesion, syntax, vocabulary, phraseology, grammar, conventions]\n",
    "\n",
    "        for t_idx, target in enumerate(target_list):\n",
    "\n",
    "            def f(x):\n",
    "                pred = np.zeros_like(target[0])\n",
    "                for i, p in enumerate(target):\n",
    "                    pred += p * x[i]\n",
    "                score = metrics.mean_squared_error(\n",
    "                    valid_y_cv.values[:, t_idx], pred, squared=False\n",
    "                )\n",
    "                return score\n",
    "\n",
    "            init_state = np.ones((len(target))) / len(target)\n",
    "            bounds = [(0.0, 1.0)] * len(target)\n",
    "            result = minimize(f, init_state, method=\"Nelder-Mead\", bounds=bounds)\n",
    "            #print(f\"optimized_corr:{result['fun']}\")\n",
    "\n",
    "            weights = [[0] for _ in range(len(target))]\n",
    "            for i in range(len(target)):\n",
    "                weights[i] = result[\"x\"][i]\n",
    "            weights_list[t_idx].append(weights)\n",
    "            #print(f\"weights:{weights}\")\n",
    "\n",
    "    avg_weights_cohesion = np.mean(weights_list[0], axis=0)\n",
    "    avg_weights_syntax = np.mean(weights_list[1], axis=0)\n",
    "    avg_weights_vocabulary = np.mean(weights_list[2], axis=0)\n",
    "    avg_weights_phraseology = np.mean(weights_list[3], axis=0)\n",
    "    avg_weights_grammar = np.mean(weights_list[4], axis=0)\n",
    "    avg_weights_conventions = np.mean(weights_list[5], axis=0)\n",
    "\n",
    "    #print()\n",
    "    #print(f\"cohesion averaged_weights:{avg_weights_cohesion}\")\n",
    "    #print(f\"syntax averaged_weights:{avg_weights_syntax}\")\n",
    "    #print(f\"vocabulary averaged_weights:{avg_weights_vocabulary}\")\n",
    "    #print(f\"phraseology averaged_weights:{avg_weights_phraseology}\")\n",
    "    #print(f\"grammar averaged_weights:{avg_weights_grammar}\")\n",
    "    #print(f\"conventions averaged_weights:{avg_weights_conventions}\")\n",
    "\n",
    "    score_cv = []\n",
    "    #scores_cv = []\n",
    "    preds_list_valid_cv = [[] for i in range(cfg1[\"general\"][\"n_splits\"])]\n",
    "    for j, preds in enumerate(preds_list_cv):\n",
    "\n",
    "        _, valid_indices = list(skf.split(train_X, train_y))[j]\n",
    "        valid_X_cv, valid_y_cv = (\n",
    "            train_X.iloc[valid_indices].reset_index(drop=True),\n",
    "            train_y.iloc[valid_indices].reset_index(drop=True),\n",
    "        )\n",
    "        \n",
    "        # blending\n",
    "        preds_list_valid_cv[j] = np.zeros(preds[0].shape)\n",
    "        for i in range(preds.shape[0]):\n",
    "            preds_list_valid_cv[j][:, 0] += preds[i, :, 0] * avg_weights_cohesion[i]\n",
    "            preds_list_valid_cv[j][:, 1] += preds[i, :, 1] * avg_weights_syntax[i]\n",
    "            preds_list_valid_cv[j][:, 2] += preds[i, :, 2] * avg_weights_vocabulary[i]\n",
    "            preds_list_valid_cv[j][:, 3] += preds[i, :, 3] * avg_weights_phraseology[i]\n",
    "            preds_list_valid_cv[j][:, 4] += preds[i, :, 4] * avg_weights_grammar[i]\n",
    "            preds_list_valid_cv[j][:, 5] += preds[i, :, 5] * avg_weights_conventions[i]\n",
    "            \n",
    "        #score, scores = mcrmse(valid_y_cv.values, np.mean(preds, axis=0))\n",
    "        #print()\n",
    "        #print(f\"simple_mean: mcrmse_score:{score}, mcrmse_scores:{scores}\")\n",
    "        #for i, p in enumerate(preds):\n",
    "            #score, scores = mcrmse(valid_y_cv.values, p)\n",
    "            #print(f\"cfg_{i}: mcrmse_score:{score}, mcrmse_scores:{scores}\")\n",
    "            \n",
    "        score, scores = mcrmse(valid_y_cv.values, preds_list_valid_cv[j])\n",
    "        #print(f\"optimized: mcrmse_score:{score}, mcrmse_scores:{scores}\")\n",
    "        score_cv.append(score)\n",
    "        #scores_cv.append(scores)\n",
    "\n",
    "    #print()\n",
    "    #print(f\"optimized mean_cv score:{np.mean(score_cv)}\")\n",
    "    #print(f\"optimized mean_cv score:{np.mean(scores_cv, axis=0)}\")\n",
    "    return np.mean(score_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T09:11:54.353408Z",
     "iopub.status.busy": "2022-11-28T09:11:54.352905Z",
     "iopub.status.idle": "2022-11-28T09:11:54.371856Z",
     "shell.execute_reply": "2022-11-28T09:11:54.370447Z",
     "shell.execute_reply.started": "2022-11-28T09:11:54.353382Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg_list = ([cfg1, cfg2, cfg5, cfg6, cfg7, cfg1p, cfg3p, cfg4p, cfg7p, cfg2sp, cfg3sp, cfg4sp, cfg5sp, cfg8p, cfg9p] \\\n",
    "+ [cfg1sp, cfg2sp, cfg3sp, cfg4sp, cfg5sp]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T09:11:55.675039Z",
     "iopub.status.busy": "2022-11-28T09:11:55.674436Z",
     "iopub.status.idle": "2022-11-28T09:11:55.681598Z",
     "shell.execute_reply": "2022-11-28T09:11:55.680777Z",
     "shell.execute_reply.started": "2022-11-28T09:11:55.675015Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg_list_retrieve = cfg_list.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T09:11:56.410513Z",
     "iopub.status.busy": "2022-11-28T09:11:56.410010Z",
     "iopub.status.idle": "2022-11-28T09:13:08.888654Z",
     "shell.execute_reply": "2022-11-28T09:13:08.887803Z",
     "shell.execute_reply.started": "2022-11-28T09:11:56.410490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_score: 0.44411874147652836\n",
      "retrieve:  deberta_v3_base_512_reinit\n",
      "score:  0.4440368526144952\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [59]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m cfg_list_repeat \u001b[38;5;241m=\u001b[39m cfg_list_retrieve\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m cfg_list_repeat[i]\n\u001b[0;32m----> 7\u001b[0m repeat_score[i] \u001b[38;5;241m=\u001b[39m \u001b[43mchoose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg_list_repeat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrieve: \u001b[39m\u001b[38;5;124m\"\u001b[39m, cfg_list_retrieve[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneral\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave_name\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore: \u001b[39m\u001b[38;5;124m\"\u001b[39m, repeat_score[i])\n",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36mchoose\u001b[0;34m(cfg_list)\u001b[0m\n\u001b[1;32m     50\u001b[0m init_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;28mlen\u001b[39m(target))) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(target)\n\u001b[1;32m     51\u001b[0m bounds \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m)] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(target)\n\u001b[0;32m---> 52\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNelder-Mead\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#print(f\"optimized_corr:{result['fun']}\")\u001b[39;00m\n\u001b[1;32m     55\u001b[0m weights \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(target))]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/scipy/optimize/_minimize.py:680\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    677\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m standardize_bounds(bounds, x0, meth)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnelder-mead\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 680\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_neldermead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m                               \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpowell\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    683\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_powell(fun, x0, args, callback, bounds, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/scipy/optimize/_optimize.py:818\u001b[0m, in \u001b[0;36m_minimize_neldermead\u001b[0;34m(func, x0, args, callback, maxiter, maxfev, disp, return_all, initial_simplex, xatol, fatol, adaptive, bounds, **unknown_options)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bounds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    817\u001b[0m     xr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(xr, lower_bound, upper_bound)\n\u001b[0;32m--> 818\u001b[0m fxr \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m doshrink \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fxr \u001b[38;5;241m<\u001b[39m fsim[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/scipy/optimize/_optimize.py:496\u001b[0m, in \u001b[0;36m_wrap_scalar_function_maxfun_validation.<locals>.function_wrapper\u001b[0;34m(x, *wrapper_args)\u001b[0m\n\u001b[1;32m    494\u001b[0m ncalls[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# A copy of x is sent to the user function (gh13740)\u001b[39;00m\n\u001b[0;32m--> 496\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwrapper_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# Ideally, we'd like to a have a true scalar returned from f(x). For\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# backwards-compatibility, also allow np.array([1.3]),\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# np.array([[1.3]]) etc.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36mchoose.<locals>.f\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(target):\n\u001b[1;32m     44\u001b[0m     pred \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m p \u001b[38;5;241m*\u001b[39m x[i]\n\u001b[0;32m---> 45\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_y_cv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     47\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_regression.py:446\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    442\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m _check_reg_targets(\n\u001b[1;32m    443\u001b[0m     y_true, y_pred, multioutput\n\u001b[1;32m    444\u001b[0m )\n\u001b[1;32m    445\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m--> 446\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m squared:\n\u001b[1;32m    449\u001b[0m     output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(output_errors)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36maverage\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/numpy/lib/function_base.py:518\u001b[0m, in \u001b[0;36maverage\u001b[0;34m(a, axis, weights, returned, keepdims)\u001b[0m\n\u001b[1;32m    515\u001b[0m     keepdims_kw \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims}\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 518\u001b[0m     avg \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkeepdims_kw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    519\u001b[0m     scl \u001b[38;5;241m=\u001b[39m avg\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype(a\u001b[38;5;241m.\u001b[39msize\u001b[38;5;241m/\u001b[39mavg\u001b[38;5;241m.\u001b[39msize)\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/numpy/core/_methods.py:182\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    180\u001b[0m ret \u001b[38;5;241m=\u001b[39m umr_sum(arr, axis, dtype, out, keepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m--> 182\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrue_divide\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m            \u001b[49m\u001b[43mret\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrcount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mret\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munsafe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_float16_result \u001b[38;5;129;01mand\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m         ret \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype(ret)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "current_score = choose(cfg_list)\n",
    "print(f\"current_score: {current_score}\")\n",
    "repeat_score = [[] for _ in range(len(cfg_list))]\n",
    "for i in range(len(cfg_list_retrieve)):\n",
    "    cfg_list_repeat = cfg_list_retrieve.copy()\n",
    "    del cfg_list_repeat[i]\n",
    "    repeat_score[i] = choose(cfg_list_repeat)\n",
    "    print(\"retrieve: \", cfg_list_retrieve[i][\"general\"][\"save_name\"])\n",
    "    print(\"score: \", repeat_score[i])\n",
    "current_score_ = np.min(repeat_score)\n",
    "retrieve_idx = np.argmin(repeat_score)\n",
    "retrieve_model = cfg_list_retrieve[retrieve_idx][\"general\"][\"save_name\"]\n",
    "print()\n",
    "print(f\"score: {current_score} -> {current_score_}\")\n",
    "print(f\"retrieve_model: {retrieve_model}, index: {retrieve_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score: 0.44365529480865284 -> 0.4435950812754264\n",
    "# retrieve_model: deberta_v3_base_512_reinit, index: 0\n",
    "# -> cfg_list = [cfg2, cfg5, cfg6, cfg7, cfg1p, cfg3p, cfg4p, cfg7p, cfg2sp, cfg3sp, cfg4sp, cfg5sp, cfg8s, cfg3s, cfg6s]\n",
    "\n",
    "# score: 0.4435950812754264 -> 0.44358263929363534\n",
    "# retrieve_model: deberta_v3_large_512_reinit_pseudo3, index: 5\n",
    "# -> cfg_list = [cfg2, cfg5, cfg6, cfg7, cfg1p, cfg4p, cfg7p, cfg2sp, cfg3sp, cfg4sp, cfg5sp, cfg8s, cfg3s, cfg6s]\n",
    "\n",
    "# score: 0.44358263929363534 -> 0.4435757923511847\n",
    "# retrieve_model: deberta_v3_large_512_reinit_svr_pseudo3, index: 9\n",
    "# -> cfg_list = [cfg2, cfg5, cfg6, cfg7, cfg1p, cfg4p, cfg7p, cfg2sp, cfg3sp, cfg5sp, cfg8s, cfg3s, cfg6s]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-27T05:44:15.091939Z",
     "iopub.status.busy": "2022-11-27T05:44:15.091279Z",
     "iopub.status.idle": "2022-11-27T05:44:15.095796Z",
     "shell.execute_reply": "2022-11-27T05:44:15.095012Z",
     "shell.execute_reply.started": "2022-11-27T05:44:15.091914Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg_list = [cfg1, cfg2, cfg5, cfg6, cfg7, cfg1p, cfg3p, cfg4p, cfg7p, cfg2sp, cfg3sp, cfg4sp, cfg5sp, cfg8s, cfg3s, cfg6s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-27T05:44:15.300617Z",
     "iopub.status.busy": "2022-11-27T05:44:15.300020Z",
     "iopub.status.idle": "2022-11-27T05:44:15.304071Z",
     "shell.execute_reply": "2022-11-27T05:44:15.303223Z",
     "shell.execute_reply.started": "2022-11-27T05:44:15.300593Z"
    }
   },
   "outputs": [],
   "source": [
    "added_cfg_list = [cfg2s, cfg4s, cfg7s, cfg9s, cfg10s, cfg11s, cfg12s, cfg13s, cfg14s, cfg15s]\n",
    "# 重いので除外 cfg5s, cfg1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-27T05:44:15.848495Z",
     "iopub.status.busy": "2022-11-27T05:44:15.847952Z",
     "iopub.status.idle": "2022-11-27T05:51:10.010466Z",
     "shell.execute_reply": "2022-11-27T05:51:10.009843Z",
     "shell.execute_reply.started": "2022-11-27T05:44:15.848470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "current_score: 0.44365529480865284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:38<05:43, 38.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0\n",
      "add:  bigbird_roberta_large_4096_svr\n",
      "score:  0.4437192172441923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [01:16<05:03, 37.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1\n",
      "add:  deberta_v2_xlarge_512_svr\n",
      "score:  0.44368133216181427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:53<04:25, 37.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2\n",
      "add:  deberta_v3_base_512_svr\n",
      "score:  0.44365578495344976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [02:31<03:47, 37.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3\n",
      "add:  deberta_v3_large_512_svr\n",
      "score:  0.44368648142274525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [03:09<03:09, 37.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4\n",
      "add:  deberta_xlarge_512_svr\n",
      "score:  0.44368823750067704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [03:47<02:31, 37.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5\n",
      "add:  electra_large_512_svr\n",
      "score:  0.4436717341679417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [04:25<01:53, 37.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6\n",
      "add:  funnel_xlarge_512_svr\n",
      "score:  0.44366729471435723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [05:03<01:15, 37.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7\n",
      "add:  longformer_large_4096_svr\n",
      "score:  0.4437240974876023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [05:40<00:37, 37.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8\n",
      "add:  muppet_large_512_svr\n",
      "score:  0.4437030795536943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [06:18<00:00, 37.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9\n",
      "add:  roberta_large_512_svr\n",
      "score:  0.4437242722782311\n",
      "\n",
      "score: 0.44365529480865284 -> 0.44365578495344976\n",
      "add_model: deberta_v3_base_512_svr, index: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "current_score = choose(cfg_list)\n",
    "print(f\"current_score: {current_score}\")\n",
    "repeat_score = [[] for _ in range(len(added_cfg_list))]\n",
    "for i in tqdm(range(len(added_cfg_list))):\n",
    "    cfg_list_repeat = cfg_list + [added_cfg_list[i]]\n",
    "    repeat_score[i] = choose(cfg_list_repeat)\n",
    "    print(i)\n",
    "    print(\"add: \", added_cfg_list[i][\"general\"][\"save_name\"])\n",
    "    print(\"score: \", repeat_score[i])\n",
    "current_score_ = np.min(repeat_score)\n",
    "add_idx = np.argmin(repeat_score)\n",
    "add_model = added_cfg_list[add_idx][\"general\"][\"save_name\"]\n",
    "print()\n",
    "print(f\"score: {current_score} -> {current_score_}\")\n",
    "print(f\"add_model: {add_model}, index: {add_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score: 0.4439758496123359 -> 0.4437593246604853\n",
    "#add_model: deberta_v3_large_4096_svr, index: 7\n",
    "\n",
    "#score: 0.4437593246604853 -> 0.44368171586630173\n",
    "#add_model: deberta_large_512_svr, index: 2\n",
    "# -> cfg_list = [cfg1, cfg2, cfg5, cfg6, cfg7, cfg1p, cfg3p, cfg4p, cfg7p, cfg2sp, cfg3sp, cfg4sp, cfg5sp, cfg8s, cfg3s]\n",
    "\n",
    "#score: 0.44368171586630173 -> 0.44365529480865284\n",
    "#add_model: deberta_v3_base_4096_svr\n",
    "# -> cfg_list = [cfg1, cfg2, cfg5, cfg6, cfg7, cfg1p, cfg3p, cfg4p, cfg7p, cfg2sp, cfg3sp, cfg4sp, cfg5sp, cfg8s, cfg3s, cfg6s]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#score: 0.44368171586630173 -> 0.44364656641620137\n",
    "#add_model: deberta_v2_xxlarge_512_svr, index: 3 <- あんま追加したくない、、\n",
    "# -> cfg_list = [cfg1, cfg2, cfg5, cfg6, cfg7, cfg1p, cfg3p, cfg4p, cfg7p, cfg2sp, cfg3sp, cfg4sp, cfg5sp, cfg8s, cfg3s, cfg5s]\n",
    "\n",
    "#score: 0.44364656641620137 -> 0.4436286575596469\n",
    "#add_model: deberta_v3_base_4096_svr, index: 3\n",
    "# -> cfg_list = [cfg1, cfg2, cfg5, cfg6, cfg7, cfg1p, cfg3p, cfg4p, cfg7p, cfg2sp, cfg3sp, cfg4sp, cfg5sp, cfg8s, cfg3s, cfg5s, cfg6s]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scoreが下がらなくなるまでループ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T14:22:04.975739Z",
     "iopub.status.busy": "2022-11-28T14:22:04.974921Z",
     "iopub.status.idle": "2022-11-28T14:22:04.978933Z",
     "shell.execute_reply": "2022-11-28T14:22:04.978453Z",
     "shell.execute_reply.started": "2022-11-28T14:22:04.975711Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg_list_retrieve = ([cfg2, cfg5, cfg6, cfg7, cfg1p, cfg4p, cfg7p, cfg2sp, cfg3sp, cfg5sp, cfg8s, cfg3s, cfg6s] + [cfg6sp, cfg7sp, cfg8sp, cfg9sp, cfg10sp, cfg11sp]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T09:13:25.218065Z",
     "iopub.status.busy": "2022-11-28T09:13:25.217485Z",
     "iopub.status.idle": "2022-11-28T09:13:25.226281Z",
     "shell.execute_reply": "2022-11-28T09:13:25.225628Z",
     "shell.execute_reply.started": "2022-11-28T09:13:25.218042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncfg_list_retrieve = ([cfg1, cfg2, cfg3, cfg4, cfg5, cfg6, cfg7] +                  [cfg1p, cfg2p, cfg3p, cfg4p, cfg5p, cfg6p, cfg8p, cfg9p, cfg10p] +                  [cfg1s, cfg2s, cfg3s, cfg4s, cfg5s, cfg6s, cfg7s, cfg8s, cfg9s, cfg10s, cfg11s, cfg12s, cfg13s, cfg14s, cfg15s] +                  [cfg1sp, cfg2sp, cfg3sp, cfg4sp, cfg5sp]).copy()\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "cfg_list_retrieve = ([cfg1, cfg2, cfg3, cfg4, cfg5, cfg6, cfg7] + \\\n",
    "                 [cfg1p, cfg2p, cfg3p, cfg4p, cfg5p, cfg6p, cfg8p, cfg9p, cfg10p] + \\\n",
    "                 [cfg1s, cfg2s, cfg3s, cfg4s, cfg5s, cfg6s, cfg7s, cfg8s, cfg9s, cfg10s, cfg11s, cfg12s, cfg13s, cfg14s, cfg15s] + \\\n",
    "                 [cfg1sp, cfg2sp, cfg3sp, cfg4sp, cfg5sp]).copy()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T14:22:07.639339Z",
     "iopub.status.busy": "2022-11-28T14:22:07.638815Z",
     "iopub.status.idle": "2022-11-28T14:48:21.750301Z",
     "shell.execute_reply": "2022-11-28T14:48:21.749607Z",
     "shell.execute_reply.started": "2022-11-28T14:22:07.639316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_score: 0.44376791841734936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [06:37<00:00, 20.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "score: 0.44376791841734936 -> 0.44366329666050924\n",
      "retrieve_model: deberta_v3_base_4096_reinit_svr_pseudo_mean_pooling_concatenate, index: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [05:51<00:00, 19.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "score: 0.44366329666050924 -> 0.44363836989695715\n",
      "retrieve_model: deberta_v3_base_512_reinit_svr_pseudo_mean_pooling_concatenate, index: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [05:07<00:00, 18.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "score: 0.44363836989695715 -> 0.44361097844447855\n",
      "retrieve_model: deberta_v3_base_4096_reinit_svr_pseudo_wap, index: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [04:26<00:00, 16.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "score: 0.44361097844447855 -> 0.4435978348259003\n",
      "retrieve_model: deberta_v3_base_4096_reinit_svr_pseudo_wap_all, index: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [03:48<00:00, 15.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "score: 0.4435978348259003 -> 0.44361031531477335\n",
      "retrieve_model: deberta_v3_base_512_reinit_pseudo3, index: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "current_score = choose(cfg_list_retrieve)\n",
    "print(f\"current_score: {current_score}\")\n",
    "\n",
    "best_score = np.inf\n",
    "while current_score < best_score:\n",
    "    best_score = current_score\n",
    "    best_cfg = cfg_list_retrieve.copy()\n",
    "    repeat_score = [[] for _ in range(len(cfg_list_retrieve))]\n",
    "    for i in tqdm(range(len(cfg_list_retrieve))):\n",
    "        cfg_list_repeat = cfg_list_retrieve.copy()\n",
    "        del cfg_list_repeat[i]\n",
    "        repeat_score[i] = choose(cfg_list_repeat)\n",
    "        #print(\"retrieve: \", cfg_list_retrieve[i][\"general\"][\"save_name\"])\n",
    "        #print(\"score: \", repeat_score[i])\n",
    "    current_score = np.min(repeat_score)\n",
    "    retrieve_idx = np.argmin(repeat_score)\n",
    "    retrieve_model = cfg_list_retrieve[retrieve_idx][\"general\"][\"save_name\"]\n",
    "    print()\n",
    "    print(f\"score: {best_score} -> {current_score}\")\n",
    "    print(f\"retrieve_model: {retrieve_model}, index: {retrieve_idx}\")\n",
    "    del cfg_list_retrieve[retrieve_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T09:06:55.130304Z",
     "iopub.status.busy": "2022-11-28T09:06:55.129832Z",
     "iopub.status.idle": "2022-11-28T09:06:55.134097Z",
     "shell.execute_reply": "2022-11-28T09:06:55.133538Z",
     "shell.execute_reply.started": "2022-11-28T09:06:55.130281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deberta_v3_base_4096_reinit\n",
      "deberta_v3_large_512_reinit\n",
      "deberta_v3_large_4096_reinit\n",
      "deberta_large_512_reinit\n",
      "muppet_large_512_reinit\n",
      "deberta_xlarge_512_reinit\n",
      "deberta_v3_base_512_reinit_pseudo3\n",
      "deberta_v3_base_4096_reinit_pseudo\n",
      "deberta_v3_large_512_reinit_pseudo3\n",
      "deberta_v3_large_4096_reinit_pseudo\n",
      "deberta_xlarge_512_reinit_pseudo\n"
     ]
    }
   ],
   "source": [
    "for i in best_cfg:\n",
    "    print(i[\"general\"][\"save_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T14:11:44.712938Z",
     "iopub.status.busy": "2022-11-28T14:11:44.712258Z",
     "iopub.status.idle": "2022-11-28T14:11:44.715926Z",
     "shell.execute_reply": "2022-11-28T14:11:44.715407Z",
     "shell.execute_reply.started": "2022-11-28T14:11:44.712912Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg_list_base = ([cfg2, cfg5, cfg6, cfg7, cfg1p, cfg4p, cfg7p, cfg2sp, cfg3sp, cfg5sp, cfg8s, cfg3s, cfg6s] + [cfg6sp, cfg7sp, cfg8sp, cfg9sp, cfg10sp, cfg11sp]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T14:12:04.465216Z",
     "iopub.status.busy": "2022-11-28T14:12:04.464924Z",
     "iopub.status.idle": "2022-11-28T14:12:04.469299Z",
     "shell.execute_reply": "2022-11-28T14:12:04.468406Z",
     "shell.execute_reply.started": "2022-11-28T14:12:04.465194Z"
    }
   },
   "outputs": [],
   "source": [
    "added_cfg_list = [cfg6sp, cfg7sp, cfg8sp, cfg9sp, cfg10sp, cfg11sp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T10:14:58.386187Z",
     "iopub.status.busy": "2022-11-28T10:14:58.385934Z",
     "iopub.status.idle": "2022-11-28T10:14:58.390785Z",
     "shell.execute_reply": "2022-11-28T10:14:58.390167Z",
     "shell.execute_reply.started": "2022-11-28T10:14:58.386168Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nadded_cfg_list = ([cfg1, cfg2, cfg3, cfg4, cfg5, cfg6, cfg7] +                  [cfg1p, cfg2p, cfg3p, cfg4p, cfg5p, cfg6p, cfg8p, cfg9p, cfg10p] +                  [cfg1s, cfg2s, cfg3s, cfg4s, cfg5s, cfg6s, cfg7s, cfg8s, cfg9s, cfg10s, cfg11s, cfg12s, cfg13s, cfg14s, cfg15s] +                  [cfg1sp, cfg2sp, cfg3sp, cfg4sp, cfg5sp]).copy()\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "added_cfg_list = ([cfg1, cfg2, cfg3, cfg4, cfg5, cfg6, cfg7] + \\\n",
    "                 [cfg1p, cfg2p, cfg3p, cfg4p, cfg5p, cfg6p, cfg8p, cfg9p, cfg10p] + \\\n",
    "                 [cfg1s, cfg2s, cfg3s, cfg4s, cfg5s, cfg6s, cfg7s, cfg8s, cfg9s, cfg10s, cfg11s, cfg12s, cfg13s, cfg14s, cfg15s] + \\\n",
    "                 [cfg1sp, cfg2sp, cfg3sp, cfg4sp, cfg5sp]).copy()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T14:19:16.196305Z",
     "iopub.status.busy": "2022-11-28T14:19:16.195663Z",
     "iopub.status.idle": "2022-11-28T14:21:01.814918Z",
     "shell.execute_reply": "2022-11-28T14:21:01.814057Z",
     "shell.execute_reply.started": "2022-11-28T14:19:16.196280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_score: 0.4435757923511847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [01:31<00:00, 15.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "score: 0.4435757923511847 -> 0.4436024923441589\n",
      "add_model: deberta_v3_base_4096_reinit_svr_pseudo_wap, index: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "current_score = choose(cfg_list_base)\n",
    "print(f\"current_score: {current_score}\")\n",
    "\n",
    "best_score = np.inf\n",
    "while current_score < best_score:\n",
    "    best_score = current_score\n",
    "    best_cfg = cfg_list_base.copy()\n",
    "    repeat_score = [[] for _ in range(len(added_cfg_list))]\n",
    "    for i in tqdm(range(len(added_cfg_list))):\n",
    "        cfg_list_repeat = cfg_list_base + [added_cfg_list[i]]\n",
    "        repeat_score[i] = choose(cfg_list_repeat)\n",
    "        #print(i)\n",
    "        #print(\"add: \", added_cfg_list[i][\"general\"][\"save_name\"])\n",
    "        #print(\"score: \", repeat_score[i])\n",
    "    current_score = np.min(repeat_score)\n",
    "    add_idx = np.argmin(repeat_score)\n",
    "    add_model = added_cfg_list[add_idx][\"general\"][\"save_name\"]\n",
    "    print()\n",
    "    print(f\"score: {best_score} -> {current_score}\")\n",
    "    print(f\"add_model: {add_model}, index: {add_idx}\")\n",
    "    cfg_list_base.append(added_cfg_list[add_idx])\n",
    "    del added_cfg_list[add_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-28T10:38:43.980060Z",
     "iopub.status.busy": "2022-11-28T10:38:43.979126Z",
     "iopub.status.idle": "2022-11-28T10:38:43.985533Z",
     "shell.execute_reply": "2022-11-28T10:38:43.984772Z",
     "shell.execute_reply.started": "2022-11-28T10:38:43.980023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deberta_v3_base_512_reinit\n",
      "deberta_v3_base_4096_reinit\n",
      "deberta_large_512_reinit\n",
      "muppet_large_512_reinit\n",
      "deberta_xlarge_512_reinit\n",
      "deberta_v3_base_512_reinit_pseudo3\n",
      "deberta_v3_large_512_reinit_pseudo3\n",
      "deberta_v3_large_4096_reinit_pseudo\n",
      "deberta_xlarge_512_reinit_pseudo\n",
      "deberta_v3_base_512_reinit_svr_pseudo\n",
      "deberta_v3_large_4096_reinit_svr_pseudo\n",
      "deberta_v3_large_512_reinit_svr_pseudo3\n",
      "deberta_xlarge_512_reinit_svr_pseudo\n",
      "deberta_v3_large_4096_svr\n",
      "deberta_large_512_svr\n",
      "deberta_v2_xxlarge_512_svr\n",
      "deberta_v3_base_4096_svr\n"
     ]
    }
   ],
   "source": [
    "for i in best_cfg:\n",
    "    print(i[\"general\"][\"save_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
